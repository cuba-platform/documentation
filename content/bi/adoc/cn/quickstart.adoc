[[quick_start]]
== Quick Start

We have created a small project to demonstrate how to enrich your CUBA application with BI reports. This demo application for customers and orders management contains an embedded Saiku report running on Pentaho server.

[[qs_setup_sample]]
=== Setting Up Sample CUBA Application

. Clone or download the sample project from https://github.com/cuba-platform/cuba-bi-demo.git

. Open the project in an IDE or in CUBA Studio, run the `createDb` command and start the application server.

[[qs_load_data]]
=== Loading Data to Star Schema

In our Pentaho report we will use aggregated data from multiple database tables. This data will be stored in additional tables and then loaded to the Star Schema. In our case, the Star Schema will consist of one fact table (Orders) and two dimension tables (Customer and Product), providing the ability to drill into the report hierarchy.

[[qs_load_data_for_impatient]]
==== Loading Data For the Impatient

You can use the ready star schema delivered with the demo project.

. Make sure you have the demo project opened in Studio . The import procedure explained below will access the project's HSQL database.

. Run Pentaho Data Integration tool:
+
--
* Navigate to the directory where Pentaho Data Integration is installed.

* Run `spoon.bat`.
--

. Open `$BI_DEMO_PROJECT/demo/kettle/bidemo.kjb`, where `$BI_DEMO_PROJECT` is the directory where the demo project is located.

. Click *Run* to update the Star Schema.
+
image::bi_star_schema.png[]

Now you can proceed to <<qs_analysis_report,creating the analysis report>>.

[[qs_db_connection]]
==== Create Database Connection

If you want to create the star schema yourself, follow the steps below. For the detailed instructions consult the http://wiki.pentaho.com/display/EAI/.03+Database+Connections[Pentaho wiki].

. Start _Pentaho Data Integration_ using `spoon.bat` from `$PENTAHO_HOME$/design-tools/data-integration`.

. Create new transformation.

. Create new database connection for the transformation:
+
--
* Enter the Connection Name

* Connection Type: Hypersonic

* Access: Native (JDBC)

* Host Name: localhost

* Database Name: bidemo

* Port Number: 19001

* User Name: sa

* Leave the Password field empty

image::star-schema.png[]
--

[[qs_dimensions]]
==== Create Dimensions

For the dimensions, we will use Products and Customers. Each product refers to some product line, that is the type of product, e.g. _Ford T_ belongs to the _Vintage Cars_ product line.

Customers belong to certain cities, they, in turn, refer to some countries, the countries are grouped into territories.

. First, create the Product transformation. Drag and drop the *Table input* node onto the worksheet and define the fields we need for the report: product `id`, `name` and `product_line_id`.
+
image::star-schema_2.png[]

. Then create an Insert/Update node for products:
+
image::star-schema_3.png[]

. Create the transformation for product lines:
+
image::star-schema_4.png[]

. Finalize the first transformation with the *Update* node:
+
image::star-schema_5.png[]

. Create the Customer transformation in the same way, including City and Territory levels, and add it to the Product one:
+
image::star-schema_6.png[]

. When the transformation is ready, wrap it in the corresponding job, using the *START* and *Success* nodes and the *Abort job* exit node in case of an error:
+
image::star-schema_12.png[]

[[qs_facts]]
==== Create Facts

For the Facts measure we will take the Orders and Order Lines.

. First, create the Order Line transformation. Drag and drop the *Table input* node onto the worksheet and define the fields we need for the report: `id`, `product_id`, `quantity` and `order_id`:
+
image::star-schema_7.png[]

. Then create an Insert/Update node for order lines:
+
image::star-schema_8.png[]

. Create the transformation for orders:
+
image::star-schema_9.png[]

. Finally, update the customers IDs in the table:
+
image::star-schema_10.png[]

. The Facts transformation is now ready:
+
image::star-schema_11.png[]

. Wrap the transformation in the corresponding job:
+
image::star-schema_13.png[]

[[qs_star_schema]]
==== Create Star Schema

Now let's assemble the dimensions and facts jobs into the complete star schema:

. Add the *START* node to start the job.

. Start the job with the *Check Db connections* condition.

. In case the DB is not connected, add the *Abort job* node for the job.

. Then add consequently *Update Dimensions* and *Update Facts* jobs that we have designed earlier.

. Finalize the job with the *Success* node and run the job:
+
image::star-schema_14.png[]

. Save all the job and transformation files in the project folder for further use.

[[qs_analysis_report]]
=== Configure Pentaho Analysis Report

. Open Pentaho console: `++http://localhost:18081/pentaho++` and login as `Admin/password`.

. Click *File → Manage Data Sources*.

. Click the settings button and select *New Connection*:
+
image::bi_pentaho.png[]

. Create connection to HSQLDB:
+
--
* Host Name: `localhost`
* Database Name: `bidemo`
* Port Number: `19001`
* User Name: `sa`

image::bi_pentaho_2.png[]
--

Now you can either use the analysis report <<qs_demo_report,delivered with the demo project>> or create it yourself following the steps <<qs_create_report,below>>.

[[qs_demo_report]]
==== Using Demo Report

Below is the easiest way to see what the Saiku report looks like, you only have to import ZIP-files with the analysis and the report structure.

. Click *Import Analysis*.

. Select `BIDemo` Data Source and import `$BI_DEMO_PROJECT/demo/pentaho/BiDemo.zip` Mondrian File. The report structure will be imported.
+
image::bi_pentaho_3.png[]

. Click New → Saiku Analytics → Create a new query. Select `BiDemo` cube and fill measure, columns, and rows as on the screen:
+
image::bi_pentaho_5.png[]

. Save report in the directory `/home/admin` with the name `ProductsByTypeAndLocation`.

Now you can open the Saiku report in the <<bi_widget,CUBA application>>.

[[qs_create_report]]
==== Create Data Source and Analysis Report Manually

Create Data Source::
+
--
. Click *New Data Source*.

. Select the Source Type: *Database Table(s)*.

. Select the new `BIDemo` connection in the list of available connections.

. Select *Reporting and Analysis* as the aim of this data source.
+
image::pentaho_console.png[]

. Select the dimensions and the fact tables we have created in Spoon: `"PENTAHO_DIM_CUSTOMER"`, `"PENTAHO_DIM_PRODUCT"`, `"PENTAHO_FACT_ORDER_LINE"`:
+
image::pentaho_console_2.png[]

. Define Joins for selected tables:
+
image::pentaho_console_3.png[]

. Customize the dimensions hierarchy:
+
image::pentaho_console_5.png[]

. Save the data source. Select it in the list of available datasources and export the created analysis for the further use:
+
image::pentaho_console_4.png[]
--

Create Analysis Report::
+
--
. Click New → Saiku Analytics → Create a new query. Select `BiDemo` cube and fill measure, columns, and rows as on the screen:
+
image::bi_pentaho_5.png[]

. Save report in the directory `/home/admin` with the name `ProductsByTypeAndLocation`.

Now you can open the Pentaho report in the <<bi_widget,CUBA application>>.
--

[[bi_widget]]
=== Open BI Widget in CUBA Sample Application

. Go to `++http://localhost:8080/app++`

. Open *Shop → BI Saiku* main menu item:

image::saiku.gif[]


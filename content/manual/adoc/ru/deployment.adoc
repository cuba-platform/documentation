[[deployment]]
== Развертывание приложений

В данной главе рассматриваются различные аспекты развертывания и эксплуатации CUBA-приложений.

На диаграмме ниже приведена возможная структура развернутого приложения. В приведенном варианте приложение обеспечивает отсутствие единой точки отказа, балансировку нагрузки и подключение различных типов клиентов.

image::DeploymentStructure.png[align="center"]

В простейшем случае, однако, приложение может быть установлено на одном компьютере, содержащем, в том числе, и базу данных. Различные варианты развертывания в зависимости от нагрузки и требований к отказоустойчивости подробно рассмотрены в <<scaling,Масштабирование приложения>>.

[[app_home]]
=== Домашний каталог приложения

Домашний каталог - это каталог файловой системы, в котором могут размещаться <<app_dirs>>. Он используется во всех <<deployment_variants,вариантах развертывания>> кроме <<fast_deployment,быстрого развертывания в Tomcat>>. В последнем случае каталоги приложения располагаются в специфичных каталогах Tomcat.

Домашний каталог формируется просто путем указания общего корня для каталогов приложения. Обычно это делается в файле `/WEB-INF/local.app.properties` внутри WAR или UberJAR.

* При сборке WAR-файла необходимо задать путь к домашнему каталогу приложения в задаче Gradle <<build.gradle_buildWar,buildWar>>. Если вы заранее знаете где будет развернут WAR, то можно указать абсолютный путь или путь относительно рабочего каталога сервера. В противном случае можно указать placeholder для системного свойства Java и передать реальный путь во время запуска.
+
--
Пример указания домашнего каталога в runtime:

** Конфигурация задачи сборки:
+
[source,groovy]
----
task buildWar(type: CubaWarBuilding) {
    appHome = '${app.home}'
    // ...
}
----

** Содержимое `/WEB-INF/local.app.properties` после сборки WAR:
+
[source,plain]
----
cuba.logDir = ${app.home}/logs
cuba.confDir = ${app.home}/${cuba.webContextName}/conf
cuba.tempDir = ${app.home}/${cuba.webContextName}/temp
cuba.dataDir = ${app.home}/${cuba.webContextName}/work
...
----

** Командная строка, задающая системное свойство `app.home`:
+
[source,plain]
----
java -Dapp.home=/opt/app_home ...
----
+
Способ указания системных свойств Java зависит от используемого сервера приложения. В случае Tomcat рекомендуется задавать их в файле `bin/setenv.sh` (или `bin/setenv.bat`).

** Результирующая структура каталогов:
+
[source,plain]
----
/opt/app_home/
  app/
    conf/
    temp/
    work/
  app-core/
    conf/
    temp/
    work/
  logs/
----
--

* В случае UberJAR, домашним каталогом по умолчанию является рабочий каталог приложения, но он может быть также задан системным свойством `app.home`. Таким образом, для установки домашнего каталога в тот же путь, что и в примере для WAR выше, достаточно сформировать следующую командную строку запуска приложения:
+
[source,plain]
----
java -Dapp.home=/opt/app_home -jar app.jar
----

[[app_dirs]]
=== Каталоги приложения

В данном разделе описываются каталоги файловой системы, используемые различными <<app_tiers,блоками приложения>> во время выполнения.

[[conf_dir]]
==== Конфигурационный каталог

Каталог конфигурации предназначен для размещения ресурсов, дополняющих и переопределяющих конфигурацию, пользовательский интерфейс и бизнес-логику после развертывания приложения. Переопределение обеспечивается механизмом загрузки интерфейса инфраструктуры `<<resources,Resources>>`, который сначала выполняет поиск в конфигурационном каталоге, а потом в classpath, так что одноименные ресурсы в конфигурационном каталоге имеют приоритет над расположенными в JAR-файлах и каталогах классов.

Конфигурационный каталог может содержать следующие типы ресурсов:

* Конфигурационные файлы <<metadata.xml,metadata.xml>>, <<persistence.xml,persistence.xml>>, <<views.xml,views.xml>>, <<remoting-spring.xml,remoting-spring.xml>>.

* <<screen_xml,XML-дескрипторы>> экранов UI.

* <<screen_controller,Контроллеры>> экранов UI в виде исходных текстов Java или Groovy.

* Скрипты или классы Groovy, а также исходные тексты классов Java, используемые приложением через интерфейс <<scripting,Scripting>>.

Расположение конфигурационного каталога определяется свойством приложения <<cuba.confDir,cuba.confDir>>. В случае <<fast_deployment,быстрого развертывания>> в Tomcat это подкаталог с именем веб-приложения в каталоге `tomcat/conf`, например `tomcat/conf/app-core` для Middleware. Для других вариантов развертывания конфигурационный каталог размещается внутри <<app_home,домашнего каталога>> приложения.

[[work_dir]]
==== Рабочий каталог

Рабочий каталог используется приложением для хранения файлов данных и конфигурации.

Например, подкаталог `filestorage` рабочего каталога по умолчанию используется <<file_storage,хранилищем загруженных файлов>>. Кроме того, блок Middleware на старте сохраняет в рабочем каталоге сгенерированные файлы <<persistence.xml,persistence.xml>> и `orm.xml`.

Расположение рабочего каталога определяется свойством приложения <<cuba.dataDir,cuba.dataDir>>. В случае <<fast_deployment,быстрого развертывания>> в Tomcat это подкаталог с именем веб-приложения в каталоге `tomcat/work`. Для других вариантов развертывания рабочий каталог размещается внутри <<app_home,домашнего каталога>> приложения.

[[log_dir]]
==== Каталог журналов

Состав и настройка файлов журналов определяются конфигурацией фреймворка *Logback*. По умолчанию в приложении используется файл конфигурации `logback.xml`, предоставляемый платформой в корне classpath. В соответствии с его настройками, вывод лога осуществляется в  standard output.

Для того чтобы указать собственную конфигурацию, необходимо передать системное свойство Java <<logback.configurationFile,logback.configurationFile>> с путем к файлу конфигурации. В разделе <<logging_setup_tomcat>> приведена информации по настройке в случае <<fast_deployment,быстрого развертывания>>.

Содержимое `logback.xml` определяет, где будут расположены файлы логов. Это может быть подкаталог внутри некоторого каталога Tomcat (`tomcat/logs` в случае быстрого развертывания), или подкаталог <<app_home,домашнего каталога>> приложения. Управлять расположением можно, если взять `logback.xml` из каталога `deploy/tomcat/conf` проекта и изменить свойство `logDir`, например:

[source,xml]
----
include::{sourcesdir}/deployment/log_dir_1.xml[]
----

Приложение должно знать, где расположены файлы логов, для того, чтобы позволить администраторам просматривать и загружать их в экране *Administration > Server Log*. Поэтому установите свойство приложения <<cuba.logDir,cuba.logDir>> в тот же каталог, который задается в `logback.xml`.

См. также <<logging, Логирование>>.

[[temp_dir]]
==== Временный каталог

Данный каталог может быть использован для создания произвольных временных файлов во время выполнения приложения. Путь к временному каталогу определяется свойством приложения <<cuba.tempDir,cuba.tempDir>>. В случае <<fast_deployment,быстрого развертывания>> в Tomcat это подкаталог с именем веб-приложения в каталоге `tomcat/temp`. Для других вариантов развертывания временный каталог размещается внутри <<app_home,домашнего каталога>> приложения.

[[db_dir]]
==== Каталог скриптов базы данных

В данном каталоге развернутого блока Middleware хранится набор SQL скриптов создания и обновления БД.

Структура каталога скриптов повторяет описанную в разделе <<db_scripts,>>, но имеет один дополнительный верхний уровень, разделяющий скрипты используемых <<app_components,компонентов>> и самого приложения. Нумерация каталогов верхнего уровня определяется во время сборки проекта.

Расположение каталога скриптов БД определяется свойством приложения <<cuba.dbDir,cuba.dbDir>>. В варианте <<fast_deployment,быстрого развертывания>> в Tomcat это подкаталог `WEB-INF/db` каталога веб-приложения среднего слоя: `tomcat/webapps/app-core/WEB-INF/db`. Для других вариантов развертывания скрипты размещаются в каталоге `/WEB-INF/db` внутри WAR или UberJAR.

[[deployment_variants]]
=== Варианты развертывания

В данном разделе рассматриваются различные варианты развертывания CUBA-приложений:

* <<fast_deployment,Быстрое развертывание в Tomcat>>

* <<war_deployment,Развертывание WAR в Jetty>>

* <<wildfly_war_deployment,Развертывание WAR в WildFly>>

* <<tomcat_war_deployment,Развертывание WAR в Tomcat Windows Service>>

* <<uberjar_deployment,UberJAR>>

* <<jelastic_deployment,Облако Jelastic>>

* <<bluemix_deployment,Облако Bluemix>>

* <<heroku_deployment,Облако Heroku>>

* <<docker_deployment,Развертывание в Docker>>

[[fast_deployment]]
==== Быстрое развертывание в Tomcat

Быстрое развертывание используется по умолчанию при разработке приложения, так как обеспечивает минимальное время сборки, установки и старта приложения. Данный вариант может также использоваться и для эксплуатации приложения.

Быстрое развертывание производится с помощью задачи <<build.gradle_deploy,deploy>>, объявленной для модулей core и web в файле `build.gradle`. Перед первым выполнением `deploy` необходимо установить и проинициализировать локальный сервер Tomcat с помощью задачи <<build.gradle_setupTomcat,setupTomcat>>.

[WARNING]
====
Пожалуйста, убедитесь, что ваше окружение не содержит переменных `CATALINA_HOME`, `CATALINA_BASE` и `CLASSPATH`. Эти переменные могут вызвать проблемы с запуском сервера, которые не будут отражаться в логах. Перезагрузите компьютер после удаления переменных.
====

В результате быстрого развертывания в каталоге, задаваемом свойством `ext.tomcatDir` скрипта `build.gradle` создается следующая структура (перечислены только важные каталоги и файлы, описанные ниже):

[source, plain]
----
bin/
    setenv.bat, setenv.sh
    startup.bat, startup.sh
    debug.bat, debug.sh
    shutdown.bat, shutdown.sh

conf/
    catalina.properties
    server.xml
    logback.xml
    logging.properties
    Catalina/
        localhost/
    app/
    app-core/

lib/
    hsqldb-2.2.9.jar

logs/
    app.log

shared/
    lib/

temp/
    app/
    app-core/

webapps/
    app/
    app-core/

work/
    app/
    app-core/
----

* `bin` - каталог, содержащий средства запуска и остановки сервера Tomcat:

** `setenv.bat`, `setenv.sh` - скрипты установки переменных окружения. Эти скрипты следует использовать для установки параметров памяти JVM, указания файла конфигурации <<logging_setup_tomcat,логирования>>, настройки <<jmx_remote_access,доступа по JMX>>, параметров <<debug_setup,подключения отладчика>>.

** `startup.bat`, `startup.sh` - скрипты запуска Tomcat. Сервер стартует в отдельном консольном окне в *Windows* и в фоне в **nix*.
+
Для запуска сервера в текущем консольном окне вместо `startup.*` используйте команды
+
`> catalina.bat run`
+
`$ ./catalina.sh run`

** `debug.bat`, `debug.sh` - скрипты, аналогичные `++startup.*++`, однако запускающие Tomcat с возможностью подключения отладчика. Именно эти скрипты запускаются при выполнении задачи <<build.gradle_start,start>> скрипта сборки.

** `shutdown.bat`, `shutdown.sh` - скрипты остановки Tomcat.

* `conf` - каталог, содержащий файлы конфигурации Tomcat и развернутых в нем приложений.

** `catalina.properties` - свойства Tomcat. Для загрузки общих библиотек из каталога `shared/lib` (см. ниже) данный файл должен содержать строку:
+
[source, plain]
----
shared.loader=${catalina.home}/shared/lib/*.jar
----

** `server.xml` - описатель конфигурации Tomcat. В этом файле можно изменить порты сервера.

** `logback.xml` - описатель конфигурации <<logging_setup_tomcat,логирования>> приложений.

** `logging.properties` - описатель конфигурации логирования самого сервера Tomcat.

** `Catalina/localhost` - в этом каталоге можно разместить дескрипторы развертывания приложений <<context.xml,context.xml>>. Дескрипторы, расположенные в данном каталоге имеют приоритет над дескрипторами в каталогах `META-INF` самих приложений, что часто бывает удобно при эксплуатации системы. Например, в таком дескрипторе на уровне сервера можно указать параметры подключения к базе данных, отличные от указанных в самом приложении.
+
Дескриптор развертывания на уровне сервера должен иметь имя приложения и расширение `.xml`. То есть для создания такого дескриптора, например, для приложения `app-core`, необходимо скопировать содержимое файла `webapps/app-core/META-INF/context.xml` в файл `conf/Catalina/localhost/app-core.xml`.

** `app` - <<conf_dir,конфигурационный каталог>> приложения веб-клиента `app`.

** `app-core` - <<conf_dir,конфигурационный каталог>> приложения среднего слоя `app-core`.

* `lib` - каталог библиотек, загружаемых в _common classloader_ сервера. Эти библиотеки доступны как самому серверу, так и всем развернутым в нем веб-приложениям. В частности, в данном каталоге должны располагаться JDBC-драйверы используемых баз данных (`hsqldb-XYZ.jar`, `postgresql-XYZ.jar` и т.д.)

* `logs` - каталог <<logging,логов>> приложений и сервера. Основной лог-файл приложений - `app.log`.

* `shared/lib` - каталог библиотек, доступных всем развернутым приложениям. Классы этих библиотек загружаются в специальный _shared classloader_ сервера. Использование shared classloader задается в файле `conf/catalina.properties` как описано выше.
+
Задачи <<build.gradle_deploy,deploy>> файла сборки копируют в этот каталог все библиотеки, не перечисленные в параметре `jarNames`, то есть не специфичные для данного приложения.

* `temp/app`, `temp/app-core` - <<temp_dir,временные каталоги>> приложений веб-клиента и среднего слоя.

* `webapps` - каталог веб-приложений. Каждое приложение располагается в собственном подкаталоге в формате _exploded WAR_.
+
Задачи <<build.gradle_deploy,deploy>> файла сборки создают подкаталоги приложений с именами, указанными в параметрах `appName`, и кроме прочего копируют в их подкаталоги `WEB-INF/lib` библиотеки, перечисленные в параметре `jarNames`.

* `work/app`, `work/app-core` - <<work_dir,рабочие каталоги>> приложений веб-клиента и среднего слоя.

[[tomcat_in_prod]]
===== Использование Tomcat при эксплуатации приложения

Процедура <<fast_deployment,быстрого развертывания>> по умолчанию создает веб приложения `app` и `app-core`, работающие на локальном инстансе Tomcat на порту 8080. Это означает, что веб клиент доступен по адресу `++http://localhost:8080/app++`.

Вы можете использовать этот экземпляр Tomcat для эксплуатации приложения, просто скопировав его на сервер. После этого необходимо установить имя хоста сервера в файлах `conf/app/local.app.properties` и `conf/app-core/local.app.properties` (создайте файлы если они не существуют):

[source, plain]
----
  cuba.webHostName = myserver
  cuba.webAppUrl = http://myserver:8080/app
---- 

Кроме того, необходимо настроить подключение к production базе данных. Это можно сделать в файле <<context.xml>> веб-приложения (`webapps/app-core/META-INF/context.xml`), или скопировать этот файл в `conf/Catalina/localhost/app-core.xml` как описано в предыдущем разделе, чтобы разделить настройки соединения с БД для разработки и эксплуатации.

Базу данных для production можно создать из бэкапа той базы, которая использовалась при разработке, либо настроить автоматическое создание и обновление БД. См. <<db_update_in_prod>>.

Опциональная конфигурация::
+
--
. Если вы хотите изменить порт Tomcat или веб-контекст (последнюю часть URL после `/`), используйте *Studio*:

* Откройте проект в Studio.

* Перейдите в *Project Properties* > *Edit* > *Advanced*.

* Чтобы изменить веб-контекст, отредактируйте поле *Modules prefix*.

* Чтобы изменить порт Tomcat, отредактируйте поле *Tomcat ports* > *HTTP port*.

. Если вы хотите использовать корневой контекст (`++http://myserver:8080/++`), переименуйте каталоги `app` (или то что вы задали на предыдущем этапе) в `ROOT`
+
[source, plain]
----
tomcat/
  conf/
      ROOT/
          local.app.properties
      app-core/
          local.app.properties
  webapps/
      ROOT/
      app-core/
----
+
и используйте `/` в качестве веб контекста в файле `conf/ROOT/local.app.properties`:
+
[source, plain]
----
cuba.webContextName = /
---- 
--

[[war_deployment]]
==== Развертывание WAR в Jetty

Рассмотрим пример сборки WAR-файлов и их развертывания на сервере *Jetty*. Предполагается, что приложение использует СУБД PostgreSQL.

. Используйте страницу *Deployment settings > WAR* в Studio или вручную добавьте в конец <<build.gradle,build.gradle>> задачу сборки <<build.gradle_buildWar,buildWar>>:
+
[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_1.groovy[]
----
+
В данном случае собирается два WAR-файла, отдельно для блоков Middleware и Web Client.

. Запустите сборку, выбрав `buildWar` в диалоге *Search* в Studio, или через командную строку (подразумевается что Gradle wrapper создан заранее):
+
[source, plain]
----
gradlew buildWar
----
+
В результате в подкаталоге `build\distributions\war` проекта будут созданы файлы `app-core.war` и `app.war`.

. Создайте домашний каталог приложения на сервере, например, `c:\work\app_home`.

. Загрузите и установите сервер Jetty, например в каталог `c:\work\jetty-home`. Данный пример тестировался на версии `jetty-distribution-9.3.6.v20151106.zip`.

. Создайте каталог `c:\work\jetty-base`, откройте в нем командную строку и выполните:
+
[source, plain]
----
java -jar c:\work\jetty-home\start.jar --add-to-start=http,jndi,deploy,plus,ext,resources
----

. Создайте файл `c:\work\jetty-base\app-jetty.xml` следующего содержания (для БД PostgreSQL с именем `test`):
+
[source, xml]
----
include::{sourcesdir}/deployment/warDeployment_2.xml[]
----

. Добавьте следующий текст в начало файла `c:\work\jetty-base\start.ini`:
+
[source, plain]
----
include::{sourcesdir}/deployment/warDeployment_3.ini[]
----

. Скопируйте JDBC-драйвер используемой базы данных в каталог `c:\work\jetty-base\lib\ext`. Файл драйвера можно взять из каталога `lib` CUBA Studio, либо из каталога `build\tomcat\lib` проекта. В случае PostgreSQL это файл `postgresql-9.1-901.jdbc4.jar`.

. Скопируйте файлы WAR в каталог `c:\work\jetty-base\webapps`.

. Откройте командную строку в каталоге `c:\work\jetty-base` и выполните:
+
[source, plain]
----
java -jar c:\work\jetty-home\start.jar
----

. Откройте `++http://localhost:8080/app++` в веб-браузере.

[[wildfly_war_deployment]]
==== Развертывание WAR в WildFly

WAR-файлы с приложением CUBA можно разворачивать на сервере *WildFly*. Рассмотрим пример сборки WAR-файлов для приложения, использующего PostgreSQL 9.6, и их развертывания на сервере WildFly версии 8.2 под Windows.

. Соберите приложение и выполните *Run* - *Deploy*, чтобы получить локальную инсталляцию Tomcat, в которой будут все необходимые зависимости для приложения.

. Подготовьте <<app_home,домашний каталог>> приложения:
+
--
* Создайте каталог, который будет полностью доступен процессу сервера WildFly, например, `C:\Users\UserName\app_home`.

* Скопируйте файл `logback.xml` из `tomcat/conf` в этот каталог и отредактируйте в нём свойство `logDir` следующим образом:

[source, xml]
----
<property name="logDir" value="${app.home}/logs"/>
----
--

. Настройте конфигурацию сервера WildFly:
+
--
* Установите WildFly, например, в каталог `C:\wildfly`.

* Отредактируйте файл `C:\wildfly\bin\standalone.conf.bat`, добавив в конец следующую строку:

[source, plain]
----
set "JAVA_OPTS=%JAVA_OPTS% -Dapp.home=%USERPROFILE%/app_home -Dlogback.configurationFile=%USERPROFILE%/app_home/logback.xml"
----

Здесь мы задаём системное свойство `app.home`, содержащее домашний каталог приложения, и указываем, где находится конфигурационный файл `logback.xml`. Вместо переменной `%USERPROFILE%` можно использовать абсолютный путь.

* Сравните версии Hibernate Validator в WildFly и приложении CUBA (платформа обычно использует более свежую версию). Замените файл `C:/wildfly/modules/system/layers/base/org/hibernate/validator/main/hibernate-validator-x.y.z-sometext.jar` более новым файлом из каталога `tomcat/shared/lib`, например, `hibernate-validator-5.4.1.Final.jar`.

* Обновите номер версии указанного JAR-файла в файле `/wildfly/modules/system/layers/base/org/hibernate/validator/main/module.xml`.

* Зарегистрируйте драйвер PostgreSQL в WildFly, скопировав файл `postgresql-9.4-1201-jdbc41.jar` из каталога `tomcat/lib` в `C:\wildfly\standalone\deployments`.
+
[TIP]
====
Для WildFly 11 процедура установки драйвера PostgreSQL будет отличаться. Сначала вам необходимо отредактировать файл `module.xml` следующим образом:

[source, xml]
----
include::{sourcesdir}/deployment/wildfly-postgres.xml[]
----

Затем из папки `bin` запустите `jboss-cli` и выполните следующую команду:

[source, plain]
----
/subsystem=datasources/jdbc-driver=postgresql:add(driver-name=postgresql, driver-module-name=org.postgresql, driver-class-name=org.postgresql.Driver)
----
====
--

. Создайте JDBC Datasource:
+
--
* Запустите WildFly, выполнив `standalone.bat`.

* Откройте консоль администратора по адресу `++http://localhost:9990++`. При первом входе потребуется создать пользователя и задать пароль.

* Перейдите в раздел *Configuration - Subsystems - Datasources* и добавьте источник данных для вашего приложения:

[source, plain]
----
Name: Cuba
JNDI Name: java:/jdbc/CubaDS
JDBC Driver: postgresql
Connection URL: URL вашей БД
Username: имя пользователя БД
Password: пароль БД
----
Драйвер JDBC будет доступен в списке обнаруженных драйверов, если вы скопировали файл `postgresql-x.y.z.jar` на предыдущем шаге.

Выполните проверку соединения, нажав кнопку *Test connection*.

* Активируйте источник данных.
--

. Соберите приложение:
+
--
* Откройте вкладку *Deployment settings* > *WAR* в Studio.

* Включите флаг *Build WAR*.

* Задайте значение `${app.home}` в поле *Application home directory*.

* Включите флаг *Include JDBC driver*.

* Сохраните настройки.

* Откройте файл <<build.gradle,build.gradle>> в IDE и добавьте свойство `doAfter` для копирования дескриптора развертывания WildFly в задачу <<build.gradle_buildWar,buildWar>>:
+
[source, groovy]
----
include::{sourcesdir}/deployment/wildfly.groovy[]
----
+
[TIP]
====
Для конфигурации singleWAR задача будет отличаться:

[source, groovy]
----
include::{sourcesdir}/deployment/wildfly-singlewar.groovy[]
----

Если ваш проект также содержит модуль <<polymer_ui,Polymer>>, нужно добавить в файл `single-war-web.xml` следующую конфигурацию:

[source, xml]
----
include::{sourcesdir}/deployment/wildfly-polymer.xml[]
----
====

* В корневом каталоге проекта создайте файл `jboss-deployment-structure.xml` и добавьте в него дескриптор развертывания WildFly:

[source, xml]
----
include::{sourcesdir}/deployment/wildfly.xml[]
----

* Запустите сборку WAR-файлов с помощью задачи `buildWar`.
--

. Скопируйте файлы `app-core.war` и `app.war` из каталога `build\distributions\war` в каталог WildFly `C:\wildfly\standalone\deployments`.

. Перезапустите WildFLy.

. Приложение будет доступно по адресу `++http://localhost:8080/app++`. Логи записываются в домашний каталог приложения: `C:\Users\UserName\app_home\logs`.

[[tomcat_war_deployment]]
==== Развертывание WAR в Tomcat Windows Service

. Добавьте в конец <<build.gradle,build.gradle>> задачу сборки <<build.gradle_buildWar,buildWar>>:
+
--
[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_2.groovy[]
----

Если параметры сервера отличаются от параметров локального Tomcat, используемого для <<fast_deployment,быстрого развертывания>>, укажите соответствующие свойства приложения:

[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_3.groovy[]
----

Можно также указать отдельный `context.xml` для настройки соединения с production БД, например:

[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_4.groovy[]
----
--

. Запустите задачу `buildWar`. В результате, в каталоге `build/distibutions` проекта будут сгенерированы файлы `app.war` и `app-core.war`.
+
[source, plain]
----
gradlew buildWar
----

. Скачайте и установите Tomcat 8 Windows Service Installer.

. После установки, перейдите в подкаталог `bin` установленного сервера и запустите `tomcat8w.exe` от имени администратора.
На вкладке *Java* установите параметр *Maximum memory pool* 1024MB. Перейдите на вкладку *General* и запустите сервис.
+
image::tomcatPropeties.jpg[align="center"]

. Пропишите `-Dfile.encoding=UTF-8` в поле _Java Options_.

. Скопируйте сгенерированные файлы `app.war` и `app-core.war` в подкаталог `webapps` сервера.

. Запустите сервис Tomcat.

. Откройте `++http://localhost:8080/app++` в браузере.

[[uberjar_deployment]]
==== Развертывание UberJAR

UberJAR - это простейший способ запустить приложение CUBA в режиме эксплуатации. Вы собираете единый all-in-one JAR-файл с помощью задачи Gradle <<build.gradle_buildUberJar>> (см. также вкладку *Deployment settings > Uber JAR* в Studio) и запускаете приложение из командной строки, используя команду `java`:

[source,plain]
----
java -jar app.jar
----

Все параметры приложения определяются во время сборки, но могут быть переопределены при запуске (см. ниже). Порт веб-приложения по умолчанию - `8080`, и оно доступно по адресу `++http://host:8080/app++`. Если в проекте есть Polymer UI, он будет доступен по умолчанию по адресу `++http://host:8080/app-front++`.

Можно также собрать отдельные JAR файлы для Middleware и Web Client, и запустить их аналогично:

----
java -jar app-core.jar

java -jar app.jar
----

Порт веб-клиента по умолчанию - `8080`, он будет подключаться к middleware, использующему `localhost:8079`. Таким образом, выполнив эти две команды в двух разных терминалах Windows, вы сможете подключиться к веб-клиенту приложения по адресу `++http://localhost:8080/app++`.

Вы можете изменить параметры, определяемые во время сборки, передав свойства приложения через системные свойства Java. Кроме того, порты, имена контекстов и пути к конфигурационным файлам можно передавать в качестве аргументов командной строки.

Аргументы командной строки::
+
--
* `port` - задаёт порт, на котором будет работать встроенный HTTP-сервер. Например:
+
[source,plain]
----
java -jar app.jar -port 9090
----
+
Следует учесть, что при указании порта для блока core необходимо также задать свойство приложения <<cuba.connectionUrlList,cuba.connectionUrlList>> для клиентских блоков, указав соответствующие адрес Middleware:
+
[source,plain]
----
java -jar app-core.jar -port 7070

java -Dcuba.connectionUrlList=http://localhost:7070/app-core -jar app.jar
----

* `contextName` - имя веб-контекста для данного блока приложения. Например, чтобы получить доступ к веб-клиенту по адресу `++http://localhost:8080/sales++`, выполните следующую команду:
+
[source,plain]
----
java -jar app.jar -contextName sales
----

* `frontContextName` - имя веб-контекста для Polymer UI (имеет смысл для единого, web или portal JAR файлов).

* `portalContextName` - имя веб-контекста для модуля portal, работающего в едином JAR.

* `jettyEnvPath` - путь к файлу окружения Jetty, который переопределяет настройки, заданные внутри JAR параметром сборки `coreJettyEnvPath`. Может быть как абсолютным путём, так и относительным рабочего каталога.

* `jettyConfPath` - путь к файлу конфигурации сервера Jetty, который переопределяет настройки, заданные внутри JAR параметром сборки `webJettyConfPath/coreJettyConfPath/portalJettyConfPath`. Может быть как абсолютным путём, так и относительным рабочего каталога.
--

Домашний каталог приложения::
+
--
По умолчанию <<app_home,домашним каталогом>> является рабочий каталог приложения. Это означает, что <<app_dirs,каталоги приложения>> будут созданы в каталоге, из которого приложение запущено. Домашний каталог может быть также указан в системном свойстве `app.home`. Например, чтобы иметь домашний каталог в `/opt/app_home`, необходимо указать следующее в командной строке:

[source,plain]
----
java -Dapp.home=/opt/app_home -jar app.jar
----
--

Логирование::
+
--
Если необходимо изменить настройки логирования, заданные внутри JAR, то можно передать системное свойство Java `logback.configurationFile` с URL для загрузки внешнего конфигурационного файла, например:

[source,plain]
----
java -Dlogback.configurationFile=file:./logback.xml -jar app.jar
----

Здесь подразумевается, что файл `logback.xml` расположен в папке, из которой запускается приложение.

Для правильного задания <<log_dir,каталога логов>> убедитесь, что свойство `logDir` в `logback.xml` указывает на подкаталог `logs` домашнего каталога приложения:

[source, xml]
----
include::{sourcesdir}/deployment/log_dir_1.xml[]
----
--

Остановка приложения::
+
--
Корректно остановить приложение можно следующими способами:

* Нажав *Ctrl+C* в окне терминала, в котором работает приложение.

* Выполнив `kill <PID>` в Unix-like системе.

* Послав ключ (последовательность символов) остановки на порт, указанный в командной строке запущенного приложения. Следующие аргументы командной строки имеют отношение к остановке:

** `stopPort` - порт прослушивания и посылки ключа остановки.
** `stopKey` - ключ остановки. Если не указан, используется `SHUTDOWN`.
** `stop` - остановить другой процесс отсылкой ключа.

Например:

[source,plain]
----
# Start application 1 and listen to SHUTDOWN key on port 9090
java -jar app.jar -stopPort 9090

# Start application 2 and listen to MYKEY key on port 9090
java -jar app.jar -stopPort 9090 -stopKey MYKEY

# Shutdown application 1
java -jar app.jar -stop -stopPort 9090

# Shutdown application 2
java -jar app.jar -stop -stopPort 9090 -stopKey MYKEY
----
--

[[uberjar_https]]
===== Настройка HTTPS для UberJAR

Ниже приведен пример настройки HTTPS с самоподписанным сертификатом для развертывания UberJAR.

. Сгенерируйте ключи и сертификаты, используя встроенную JDK-утилиту `Java Keytool`:
+
[source, plain]
----
keytool -keystore keystore.jks -alias jetty -genkey -keyalg RSA
----

. В корневом каталоге проекта создайте файл конфигурации SSL `jetty.xml`:
+
[source, xml]
----
include::{sourcesdir}/deployment/uberjar-https.xml[]
----
+
Удостоверьтесь, что значения свойств `keyStorePassword`, `keyManagerPassword` и `trustStorePassword` совпадают с паролями, установленными в `Keytool`.

. Включите файл `jetty.xml` в конфигурацию задачи сборки:
+
[source, groovy]
----
include::{sourcesdir}/deployment/uberjar-https.groovy[]
----

. Соберите Uber JAR, следуя инструкции, описанной в разделе <<uberjar_deployment>>.

. Поместите файл `keystore.jks` в папку с JAR-файлами собранного приложения и запустите Uber JAR.
+
Теперь приложение доступно по адресу https://localhost:8443/app.

[[jelastic_deployment]]
==== Развертывание в облаке Jelastic

CUBA Studio позволяет легко развернуть приложение в облаке link:$$https://jelastic.com/$$[Jelastic].

[TIP]
====
В данный момент развертывание в облаке возможно только для проектов, использующих в качестве сервера базы данных PostgreSQL или HSQL.
====

. Нажмите на ссылку *Deployment settings* в секции *Project properties* и перейдите на вкладку *CLOUD*.

. Если для данного проекта еще нет настроек развертывания в облаке, вы можете использовать поле вверху открывшейся страницы для создания бесплатной тестовой учетной записи Jelastic.

. После завершения регистрации введите email, пароль и выбранного хостинг-провайдера.
+
image::jelastic_1.png[align="center"]

. В поле *Environment* вводится имя окружения, в которое будет развернут WAR. Нажмите на кнопку с троеточием и выберите существующее окружение или создайте новое. Вы можете проверить окружение на совместимость с вашим проектом. Совместимое окружение должно иметь Java 8, Tomcat 8 и PostgreSQL 9.1+ (если в проекте используется база данных PostgreSQL). Если ваш проект использует PostgreSQL, вы получите email с информацией о подключении к БД. Используйте эту информацию при генерации context.xml, см. поле *Custom context.xml path* ниже. Кроме того, вы должны создать пустую базу данных PostgreSQL через веб-интерфейс провайдера, ссылка на который содержится в письме. Выбранное имя базы данных должно быть указано позже в context.xml.
+
image::jelastic_6.png[align="center"]

. Нажмите кнопку *Generate* рядом с полем *Custom web.xml path*. Studio сгенерирует специальный `web.xml` для <<build.gradle_buildWar,единого WAR>>, содержащего блоки Middleware и Web Client.
+
image::jelastic_2.png[align="center"]

. Если проект использует HSQLDB, то это все - вы можете нажать *OK* и запустить развертывание командой *Run > Deploy to cloud* главного меню. Параметры развёртывания можно позже изменить в файле <<build.gradle_deployWar,build.gradle>>.

. Если проект использует PostgreSQL, перейдите в административный веб-интерфейс по ссылке в письме, полученном после создания Environment, и создайте базу данных.

. Нажмите кнопку *Generate* рядом с полем *Custom context.xml path* и укажите пользователя, пароль, хост и имя базы данных.
+
image::jelastic_3.png[align="center"]

. Оставьте флажки *Include JDBC driver* и *Include context.xml* включенными.
+
image::jelastic_4.png[align="center"]

. Нажмите *OK* и запустите развертывание командой *Run > Deploy to cloud* главного меню.

. После завершения развертывания используйте сссылку в левом нижнем углу чтобы открыть веб-интерфейс приложения.
+
image::jelastic_5.png[align="center"]

[[bluemix_deployment]]
==== Развёртывание в облаке Bluemix

С помощью CUBA Studio можно легко развернуть приложение в облаке IBM® Bluemix®.

[TIP]
====
Развёртывание в облаке Bluemix в настоящее время рекомендуется только для проектов, использующих базу данных PostgreSQL. HSQLDB доступна только с опцией _in-process_, таким образом, база данных будет пересоздаваться каждый раз при перезапуске облачного приложения, соответственно, пользовательские данные будут потеряны.
====

. Создайте учётную запись в сервисе Bluemix. Также скачайте и установите следующее программное обеспечение:
.. Bluemix CLI: http://clis.ng.bluemix.net/ui/home.html
.. Cloud Foundry CLI: https://github.com/cloudfoundry/cli/releases
.. После установки убедитесь, что команды `bluemix` и `cf` работают в командной строке. При необходимости добавьте путь к исполняемым файлам `\IBM\Bluemix\bin` в переменную среды `PATH`.

. Создайте новое пространство (Space) в облаке Bluemix, задайте ему любое имя. В дальнейшем вы можете поместить несколько приложений в одно пространство.

. Добавьте к созданному пространству сервер приложений Tomcat: *Create App* -> *CloudFoundry Apps* -> *Tomcat*.

. Задайте имя приложения. Имя должно быть уникальным, так как на его основе строится URL, по которому WEB-приложение будет доступно впоследствии.

. Чтобы добавить к пространству подходящий сервис базы данных, нажмите *Create service* в панели управления пространством и выберите *ElephantSQL*.

. Откройте панель управления приложением (ранее созданный Tomcat) и подключите сервис базы данных к приложению. Нажмите *Connect Existing*. Чтобы изменения вступили в силу, система предлагает обновить (restage) приложение. На данном этапе в этом нет необходимости: сервер Tomcat будет обновлен позже при развертывании CUBA-приложения.

. После подключения сервиса базы данных к приложению параметры подключения к СУБД будут доступны по кнопке *View Credentials*. Также параметры подключения к СУБД сохраняются в переменной среды `VCAP_SERVICES` облачного приложения и доступны по команде `cf&#160;env`. Созданная БД доступна глобально, управлять базой данных можно по указанному URL.

. Настройте CUBA-проект на базу данных PostgreSQL (на СУБД, аналогичную той которую Вы используете в облаке Bluemix).

. Создайте скрипты базы данных и запустите локальный сервер Tomcat. Убедитесь, что приложение работоспособно.

. Создайте WAR-файл, при помощи которого приложение будет равзернуто в сервер Tomcat.
.. Нажмите *Deployment Settings* в разделе *Project Properties* панели навигатора Studio.
.. Перейдите на вкладку *WAR*.
.. При помощи чекбоксов выберите все доступные опции: для корректного развертывания в облаке необходим единый *Single WAR* файл с помещёнными в него драйвером базы данных и конфигурационным файлом `context.xml`.
+
image::bluemix_war_settings.png[align="center"]

.. Нажмите кнопку *Generate* рядом с полем *Custom context.XML*. В появившемся диалоге укажите параметры подключения к базе данных - сервису в облаке Bluemix.
+
Используйте параметры из строки `uri` сервиса, как в примере ниже:
+
[source, json]
----
include::{sourcesdir}/deployment/bluemix_credentials.json[]
----
+
*Database user*: `ldwpelpl`
+
*Database password*: `eFwXx6lNFLheO5maP9iRbS77Sk1VGO_T`
+
*Database URL*: `echo-01.db.elephantsql.com:5432`
+
*Database name*: `ldwpelpl`

.. Нажмите кнопку *Generate* для создания собственного файла `web.xml`, необходимого для единого WAR-файла.

.. Сохраните настройки. Создайте WAR-файл, выполнив команду Gradle `buildWar` в Studio или из командной строки.
+
image::bluemix_buildWar.png[align="center"]
+
В результате, в папке проекта `build/distributions/war/` появился файл `app.war`.

. В корневом каталоге прокекта вручную создайте файл `manifest.yml` со следующим содержимым:
+
[source, yml]
----
include::{sourcesdir}/deployment/bluemix_manifest.yml[]
----
+
где
+
* `path` - относительный путь к сгенерированному WAR-файлу.
* `memory`: по умолчанию  серверу Tomcat выделяется лимит памяти в 1G. При необходимости вы можете уменьшить или увеличить объём выделенной памяти, эта настройка также доступна через WEB-интерфейс Bluemix. Учтите, что количество памяти, выделенной приложению, влияет на стоимость облачного размещения.
* `name` - имя сервера приложения Tomcat, созданного в облаке.
* `host`: идентично имени приложения.
* `env`: этим параметром задаются переменные среды. В нашем случае переменными среды задаются версии Tomcat и Java, необходимые для правильного функционирования CUBA-приложения.

. В комадной строке перейдите в корневой каталог проекта CUBA.
+
[source, yml]
----
cd your_project_directory
----

. Создайте подключение к Bluemix.
+
[source, yml]
----
bluemix api https://api.ng.bluemix.net
----

. Зайдите в Вашу учетную запись Bluemix.
+
[source, yml]
----
cf login
----

.  Разверните созданный WAR в облачный Tomcat.
+
[source, yml]
----
cf push
----
+
Команда `push` использует параметры, указанные в конфигурационном файле `manifest.yml`.

. Посмотреть логи сервера Tomcat можно на вкладке *Logs* панели управления приложением в WEB-интерфейсе Bluemix, а также в командной строке при помощи команды
+
[source, yml]
----
cf logs cuba-app --recent
----

. По завершению процесса развёртывания CUBA-приложение будет доступно в облаке Bluemix. Чтобы его открыть, воспользуйтесь URL `host.domain` в браузере. Этот URL будет отображаться в поле *ROUTE* таблицы ваших приложений *Cloud Foundry Apps*.

[[heroku_deployment]]
==== Развертывание в облаке Heroku

Данный раздел описывает порядок развертывания приложения CUBA в облаке https://www.heroku.com/[Heroku®].

[TIP]
=====
Это руководство охватывает процесс развертывания проекта с использованием базы данных PostgreSQL.
=====

[[heroku_war_deployment]]
===== Развертывание WAR-файла в Heroku

Учетная запись Heroku::
+
--
Создайте учетную запись в Heroku с помощью веб-браузера, будет достаточно бесплатного аккаунта `hobby-dev`. Затем войдите в аккаунт и создайте новое приложение с помощью кнопки *New* в верхней части страницы.

Задайте уникальное имя приложения (либо оставьте поле пустым, чтобы имя назначилось автоматически) и выберите подходящее геоположение сервера. Вы зарегистрировали приложение, например `morning-beach-4895`, это будет название приложения Heroku.

Сначала вас переадресует на вкладку *Deploy*. Выберите там метод развертывания *Heroku Git*.
--

Командная строка Heroku (CLI)::
+
--
* Установите на компьютер программное обеспечение https://devcenter.heroku.com/articles/heroku-command-line[Heroku CLI].

* Перейдите в папку проекта CUBA. В дальнейшем для этой папки будет использоваться переменная `$PROJECT_FOLDER`.

* Откройте командную строку в `$PROJECT_FOLDER` и наберите команду:
+
[source,plain]
----
heroku login
----

* По запросу введите логин и пароль для Heroku. Начиная с текущего момента от вас больше не потребуется вводить логин и пароль для команд heroku.

* Установите плагин *Heroku CLI deployment plugin*:
+
[source,plain]
----
heroku plugins:install heroku-cli-deploy
----
--

База данных PostgreSQL::
+
--
С помощью браузера пройдите на страницу https://data.heroku.com/[Heroku data]

Вы можете использовать существующую базу Postgres или создать новую. Далее описываются шаги по созданию новой БД.

* Найдите на странице блок *Heroku Postgres* и нажмите кнопку *Create one*
* На следующем экране нажмите кнопку *Install Heroku Postgr...*
* Далее подключите базу к приложению Heroku, выбрав подходящую из выпадающего списка
* Далее выберите тарифный план (например, бесплатный `hobby-dev`)

Как вариант, вы можете установить PostgreSQL с помощью Heroku CLI:

[source,plain]
----
heroku addons:create heroku-postgresql:hobby-dev --app morning-beach-4895
----

Здесь `morning-beach-4895` это название вашего приложения Heroku.

Теперь вы можете увидеть новую БД на вкладке *Resources*. База соединена с приложением Heroku. Чтобы получить детали для подключения к сервису БД, перейдите на страницу *Datasource* вашей БД в Heroku, опуститесь вниз до секции *Administration* и нажмите кнопку *View credentials*.

[source,plain]
----
Host compute.amazonaws.com
Database d2tk
User nmmd
Port 5432
Password 9c05
URI postgres://nmmd:9c05@compute.amazonaws.com:5432/d2tk
----
--

Настройки проекта перед развертыванием::
+
--
* Мы предполагаем, что в проекте CUBA вы используете базу данных Postgres.

* Откройте проект CUBA в Студии, выполните действие *Deployment settings*, перейдите на вкладку *WAR* и затем отредактируйте настройки, как описано ниже.
+
** Включите *Build WAR*
** Задайте точку '.' в качестве домашнего каталога приложения в поле *Application home directory*
** Включите *Include JDBC driver*
** Включите *Include Tomcat's context.xml*
** Нажмите кнопку *Generate*, находящуюся справа от поля *Custom context.xml path*. Во всплывающем окне заполните параметры подключения к БД
** Откройте сгенерированный файл `modules/core/web/META-INF/war-context.xml` и проверьте детали подключения:
+
[source, xml]
----
include::{sourcesdir}/deployment/war-context.xml[]
----
+
** Отметьте галочкой *Single WAR for Middleware and Web Client*
** Нажмите кнопку *Generate* справа от поля *Custom web.xml path*
** Скопируйте код, приведенный ниже, в поле *App properties*:
+
[source, groovy]
----
[
  'cuba.automaticDatabaseUpdate' : true
]
----
+
** Сохраните настройки.
--

Сборка WAR-файла::
+
--
Соберите WAR-файл, выполнив команду `buildWar` в Gradle. Вы можете сделать это прямо в Студии, открыв диалог *Search*, или из командной строки:

[source,plain]
----
gradlew buildWar
----

Проект CUBA использует Gradle wrapper (gradlew). Чтобы иметь возможность работать с командой `gradlew`, заранее создайте Gradle wrapper, использовав команду меню *Build > Create or update Gradle wrapper*.
--

Настройка приложения::
+
--

* Загрузите JAR-файл Tomcat Webapp Runner из репозитория https://mvnrepository.com/artifact/com.github.jsimone/webapp-runner. Версия Webapp Runner должна соответствовать используемой версии Tomcat. К примеру, Webapp Runner версии 8.5.11.3 подходит для Tomcat версии 8.5.11. Переименуйте JAR-файл в `webapp-runner.jar` и поместите его в корень проекта `$PROJECT_FOLDER`.

* Загрузите JAR-файл Tomcat DBCP из репозитория https://mvnrepository.com/artifact/org.apache.tomcat/tomcat-dbcp.
  Используйте версию, соответствующую вашему Tomcat, например 8.5.11. Создайте папку `$PROJECT_FOLDER/libs`, переименуйте JAR-файл в `tomcat-dbcp.jar` и поместите его в папку `$PROJECT_FOLDER/libs`.

* Создайте файл с названием `Procfile` в `$PROJECT_FOLDER`. Файл должен содержать следующий текст:
+
[source,plain]
----
web: java $JAVA_OPTS -cp webapp-runner.jar:libs/* webapp.runner.launch.Main --enable-naming --port $PORT build/distributions/war/app.war
----
--

Настройка Git::
+
--
Откройте командную строку в папке `$PROJECT_FOLDER` и запустите команды, указанные ниже:

[source,plain]
----
git init
heroku git:remote -a morning-beach-4895
git add .
git commit -am "Initial commit"
----
--

Развертывание приложения::
+
--
Откройте командную строку в папке `$PROJECT_FOLDER` и запустите команды, указанные ниже:

Для *nix:

[source,plain]
----
heroku jar:deploy webapp-runner.jar --includes libs/tomcat-dbcp.jar:build/distributions/war/app.war --app morning-beach-4895
----

Для Windows:

[source,plain]
----
heroku jar:deploy webapp-runner.jar --includes libs\tomcat-dbcp.jar;build\distributions\war\app.war --app morning-beach-4895
----

Откройте вкладку *Resources* в панели управления Heroku. Должна появиться новая запись Dyno с командой из вашего `Procfile`:

image::heroku_dyno.png[align="center"]

Приложение в данный момент разворачивается. Вы можете отслеживать процесс по логам.
--

Мониторинг логов::
+
--
Дождитесь сообщения в командной строке `++https://morning-beach-4895.herokuapp.com/  deployed to Heroku++`.

Чтобы отслеживать данные в логах, запустите в командной строке из любой папки следующую команду:

[source,plain]
----
heroku logs --tail --morning-beach-4895
----
--

После завершения процесса развертывания ваше приложение будет доступно в браузере по ссылке `++https://morning-beach-4895.herokuapp.com++`

Вы также можете открыть приложение с помощью кнопки *Open app*, расположенной на панели Heroku.

[[heroku_github_deployment]]
===== Развертывание из GitHub в Heroku

Данное руководство описывает процесс настройки сборки и развертывания проекта, размещенного на GitHub.

Учетная запись Heroku::
+
--
Создайте учетную запись в Heroku с помощью веб-браузера, будет достаточно бесплатного аккаунта `hobby-dev`. Затем войдите в аккаунт и создайте новое приложение с помощью кнопки *New* в верхней части страницы.

Задайте уникальное имя приложения (либо оставьте поле пустым, чтобы имя назначилось автоматически) и выберите подходящее геоположение сервера. Вы зарегистрировали приложение, например `space-sheep-02453`, это будет название приложения Heroku.

Сначала вас переадресует на вкладку *Deploy*. Выберите там метод развертывания *GitHub*. Следуйте инструкциям на экране, чтобы авторизоваться в учетную запись GitHub.
Нажмите кнопку *Search*, чтобы вывести список доступных репозиториев GitHub вашей учетной записи, затем подключите желаемый репозиторий к проекту CUBA. Когда приложение Heroku подсоединено к GitHub, то вам доступна функция автоматического развертывания приложения *Automatic Deploys*. Это позволяет развертывать приложение в Heroku автоматически при каждом событии `git push`. В этом руководстве данная опция включена.
--

Командная строка Heroku (CLI)::
+
--
* Установите на компьютер программное обеспечение https://devcenter.heroku.com/articles/heroku-command-line[Heroku CLI]
* Откройте командную строку в любой папке вашего компьютера и наберите команду:
+
[source,plain]
----
heroku login
----
+
* По запросу введите логин и пароль для Heroku. Начиная с текущего момента от вас больше не потребуется вводить логин и пароль для команд heroku.
--

База данных PostgreSQL::
+
--
* Откройте https://dashboard.heroku.com[панель] Heroku в веб-браузере
* Перейдите на вкладку *Resources*
* Нажмите кнопку *Find more add-ons*, чтобы найти дополнения для подключения СУБД
* Найдите блок *Heroku Postgres* и нажмите его. Проследуйте инструкциям на экране, нажмите кнопки *Login to install* / *Install Heroku Postgres* для установки дополнения.

Как вариант, вы можете установить PostgreSQL с помощью Heroku CLI, где `space-sheep-02453` - это имя вашего Heroku приложения:

[source,plain]
----
heroku addons:create heroku-postgresql:hobby-dev --app space-sheep-02453
----

Теперь вы можете увидеть новую БД на вкладке *Resources*. База соединена с приложением Heroku. Чтобы получить детали для подключения к сервису БД, перейдите на страницу *Datasource* вашей БД в Heroku, опуститесь вниз до секции *Administration* и нажмите кнопку *View credentials*.

[source,plain]
----
Host compute.amazonaws.com
Database zodt
User artd
Port 5432
Password 367f
URI postgres://artd:367f@compute.amazonaws.com:5432/zodt
----
--

Настройки проекта перед развертыванием::
+
--
* Перейдите в папку проекта CUBA (`$PROJECT_FOLDER`) на вашем компьютере
* Скопируйте содержимое файла `modules/core/web/META-INF/context.xml` в `modules/core/web/META-INF/heroku-context.xml`
* Впишите в файл `heroku-context.xml` актуальные данные для подключения в БД (см. пример ниже):
+
[source, xml]
----
include::{sourcesdir}/deployment/heroku-context.xml[]
----
--

Настройка сборки::
+
--
Добавьте следующую задачу Gradle в ваш файл `$PROJECT_FOLDER/build.gradle`

[source, groovy]
----
include::{sourcesdir}/deployment/heroku_buildGradle.groovy[]
----
--

Procfile::
+
--
Команда, которая запускает приложение в Heroku, передается через специальный файл `Procfile`. Создайте файл с названием `Procfile` в папке `$PROJECT_FOLDER`, содержащий следующий текст:

[source,plain]
----
web: cd ./deploy/tomcat/bin && export 'JAVA_OPTS=-Dport.http=$PORT' && ./catalina.sh run
----

Это передает значение переменной среды JAVA_OPTS в Tomcat, который в свою очередь запускает скрипт Catalina.
--

Премиум дополнения::
+
--
Если ваш проект использует премиальные дополнения CUBA, то укажите дополнительные переменные в приложении Heroku.

* Откройте панель Heroku в браузере
* Перейдите на вкладку *Settings*
* Разверните секцию *Config Variables*, нажав кнопку *Reveal Config Vars*
* Добавьте новые переменные *Config Vars*, используя части вашего лицензионного ключа (разделенные дефисом) как *username* и *password*:

[source]
----
CUBA_PREMIUIM_USER    | username
CUBA_PREMIUM_PASSWORD | password
----
--

Gradle wrapper::
+
--
Проект CUBA использует Gradle wrapper (gradlew). Чтобы иметь возможность работать с командой `gradlew`, заранее создайте Gradle wrapper, использовав команду меню *Build > Create or update Gradle wrapper*.

* Создайте файл `system.properties` в папке `$PROJECT_FOLDER` следующего содержания (пример соответствует локально установленной версии JDK 1.8.0_121):
+
[source,plain]
----
java.runtime.version=1.8.0_121
----
+
* Убедитесь, что файлы `Procfile`, `system.properties`, `gradlew`, `gradlew.bat` и `gradle` не включены в `.gitignore`
* Добавьте эти файлы в репозиторий и выполните коммит:

[source,plain]
----
git add gradlew gradlew.bat gradle/* system.properties Procfile
git commit -am "Added Gradle wrapper and Procfile"
----
--

Развертывание приложения::
+
--
Как только вы выполните Push изменений в GitHub, то Heroku начнет разворачивать приложение.

[source,plain]
----
git push
----

Контроль процесса развертывания осуществляется в панели Heroku на вкладке *Activity*. Перейдите по ссылке *View build log*, чтобы отслеживать лог.

После завершения процесса развертывания ваше приложение будет доступно в браузере по ссылке `++https://space-sheep-02453.herokuapp.com/++`

Вы также можете открыть приложение с помощью кнопки *Open app*, расположенной на панели Heroku.
--

Мониторинг логов::
+
--
Чтобы отслеживать данные в логах, запустите в командной строке следующую команду:

[source,plain]
----
heroku logs --tail --app space-sheep-02453
----

Логи Tomcat также доступны в веб-приложении: *Menu > Administration > Server Log*
--
[[docker_deployment]]
==== Развертывание в Docker

Данный раздел описывает порядок развертывания приложения CUBA в https://www.docker.com/[Docker®].

[TIP]
=====
Это руководство охватывает процесс развертывания проекта с использованием базы данных PostgreSQL.
=====

UberJAR - это простейший способ запустить приложение CUBA в режиме эксплуатации. Вы собираете единый all-in-one JAR-файл с помощью задачи Gradle <<build.gradle_buildUberJar>> (см. также вкладку *Deployment settings > Uber JAR* в Studio) и запускаете приложение из командной строки, используя команду `java`. Все параметры приложения определяются во время сборки, но могут быть переопределены при запуске.

В этом разделе показано, как настроить монолитную и распределенную конфигурации приложений с Docker контейнерами.

[[single_jar_deployment]]
===== Развертывание монолитного Uber JAR

Откройте проект CUBA в Студии, выполните действие *Deployment settings*, перейдите на вкладку *Uber JAR* и затем отредактируйте настройки, как описано ниже.

. Включите *Build Uber JAR*
. Включите *Single Uber JAR* если не включено.
. Нажмите кнопку *Generate* находящуюся справа от поля *Logback configuration file*.
. Нажмите кнопку *Generate* находящуюся справа от поля *Custom Jetty environment file*. Во всплывающем окне заполните параметры подключения к БД.
Для использования в приложении стандартного контейнера PostgreSQL, нужно заменить `localhost` на `postgres` в поле *Database URL*.
. Сохраните настройки.

Соберите JAR-файл, выполнив команду <<build.gradle_buildUberJar>> в Gradle. Вы можете сделать это прямо в Студии, открыв диалог *Search*, или из командной строки:

[source, plain]
----
gradle buildUberJar
----

Docker-образ с проектом  CUBA должен использовать базовый образ OpenJDK. Для указания информации, необходимой Docker для запуска приложения, используется файл `Dockerfile`. `Dockerfile` — это текстовый файл, в котором содержится список команд Docker-клиента. Это простой способ автоматизировать процесс создания образа.  В файле можно указать базовый образ Docker для запуска, местоположения кода проекта, а так же необходимые зависимости и команды, которые нужно использовать при запуске контейнера.

. Создайте в проекте папку `docker-image`.
. Скопируйте в неё JAR-файл.
. Создайте `Dockerfile`. Файл должен содержать следующий текст:

[source, plain]
----
### Dockerfile

FROM openjdk:8

COPY . /usr/src/cuba-sales

CMD java -Dapp.home=/usr/src/cuba-sales/home -jar /usr/src/cuba-sales/app.jar
----

* Инструкция `FROM` инициализирует новый этап сборки и устанавливает базовый образ для последующих инструкций.
* Инструкция `COPY` копирует новые файлы или директории из `<src>` и добавляет их в файловую систему контейнера по пути `<dest>`.
  Можно указать несколько ресурсов `<src>` , но они должны относиться к исходному каталогу, в котором запушена сборка.
* Главное предназначение инструкции `CMD` — сообщить контейнеру какие команды нужно выполнить при старте.

Для получения дополнительной информации об инструкциях `Dockerfile` см. https://docs.docker.com/engine/reference/builder/[Dockerfile reference].

Теперь можно создать образ:

. Откройте терминал из папки `docker-image`.
. Запустите команду сборки. Команда `docker build` довольно проста: она принимает опциональный тег с флагом `-t` и путь до директории, в которой лежит `Dockerfile`, Точка `.` означает текущую директорию.

[source, plain]
----
docker build -t cuba-sample-sales .
----

Если у вас нет образа `openjdk:8` то Docker-клиент сначала скачает его, а потом создаст образ приложения CUBA.

Для определения и запуска много-контейнерных Docker приложений используется инструмент https://docs.docker.com/compose/overview/[Docker Compose]. Вся конфигурация для docker-compose описывается в файле `docker-compose.yml`, и с его помощью можно одной командой поднять приложение с необходимым набором сервисов.

Конфигурационный файл будет иметь следующую сруктуру:

[source, yml]
----
version: '2'

services:
  postgres:
    image: postgres:9.6.6
    environment:
      - POSTGRES_PASSWORD=cuba
      - POSTGRES_USER=cuba
      - POSTGRES_DB=sales
    ports:
     - "5433:5432"
    networks:
     - sales-network
  web:
    image: cuba-sample-sales
    ports:
     - "8080:8080"
    networks:
     - sales-network

networks:
  sales-network:
----

В этом файле определены два сервиса, `web` and `postgres`. Сервис `web`:

* использует образ приложения CUBA, который был создан с помощью `Dockerfile`,
* пробрасывает порт 8080 контейнера на порт 8080 хоста.

Сервис `postgres` использует публичный образ Postgres, скачанный из репозитория Docker Hub.

Для запуска приложения перейдите в директорию с файлом `docker-compose.yml` и выполните команду:

[source, plain]
----
docker-compose up
----

Приложение будет доступно по адресу `++http://localhost:8080/app++`.

===== Distributed Uber JAR Deployment

Для настройки распределенной конфигурации откройте проект CUBA в Студии, выполните действие *Deployment settings*, перейдите на вкладку *Uber JAR* и затем отредактируйте настройки, как описано ниже:

. Включите *Build Uber JAR*
. Выключите *Single Uber JAR*.

Добавьте следующие настройки в appProperties:

[source, plain]
----
appProperties = ['cuba.automaticDatabaseUpdate': true,
                 'cuba.webHostName':'sales-core',
                 'cuba.connectionUrlList': 'http://sales-core:8079/app-core',
                 'cuba.webAppUrl': 'http://sales-web:8080/app',
                 'cuba.useLocalServiceInvocation': false,
                 'cuba.trustedClientPermittedIpList': '*.*.*.*']
----

* Свойство <<cuba.webHostName,cuba.webHostName>>  -- конфигурационный параметр, задающий имя хоста, на котором запущен данный блок приложения. Указанное значение должно совпадать с именем core сервиса, указанного в `Dockerfile`.
* Свойство <<cuba.connectionUrlList,cuba.connectionUrlList>> задает список URL для подключения клиентских блоков к серверам Middleware. Имя хоста должно совпадать с именем сервиса `core`, указанного в `Dockerfile`, а  contextName должен соответствовать имени файла core *.jar.
* Свойство <<cuba.webAppUrl,cuba.webAppUrl>> определяет URL, по которому доступен Web Client приложения. Имя хоста должно совпадать с именем сервиса `web`, указанного в `Dockerfile`.
* Свойство <<cuba.useLocalServiceInvocation,cuba.useLocalServiceInvocation>> должно быть установлено в `false`, так как блоки Middleware и Web Client развернуты в отдельных контейнерах.
* Свойство <<cuba.trustedClientPermittedIpList,cuba.trustedClientPermittedIpList>> определяет список IP адресов, с которых возможен вход в приложение.

[TIP]
====
Если в приложении используется более одного сервера Middleware, их все нужно перечислитьв свойстве <<cuba.connectionUrlList,cuba.connectionUrlList>> и настроить кластер серверов Web Client как описано в разделе <<scaling>>.
====

Пересоберите JAR-файлы с помощью задачи <<build.gradle_buildUberJar>>:

[source, plain]
----
gradle buildUberJar
----

Создайте две подпапки в папке `docker-image` для web и core JAR-файлов. Так как для распределенного приложения создается отдельный контейнер для каждого JAR-файла, то в этом случае необходимо конфигурировать два файла `Dockerfile`.

Конфигурационный файл `Dockerfile` для *core* имеет вид:

[source, plain]
----
### Dockerfile

FROM openjdk:8

COPY . /usr/src/cuba-sales

CMD java -Dapp.home=/usr/src/cuba-sales/home -jar /usr/src/cuba-sales/app-core.jar
----

Конфигурационный файл `Dockerfile` для *web* имеет вид:

[source, plain]
----
### Dockerfile

FROM openjdk:8

COPY . /usr/src/cuba-sales

CMD java -Dapp.home=/usr/src/cuba-sales/home -jar /usr/src/cuba-sales/app.jar
----

Файл `docker-compose.yml` конфигурирует два сервиса *core* и *web* и выглядит следующим образом:

[source, yml]
----
version: '2'

services:
  postgres:
    image: postgres:9.6.6
    environment:
      - POSTGRES_PASSWORD=cuba
      - POSTGRES_USER=cuba
      - POSTGRES_DB=sales
    ports:
     - "5433:5432"
    networks:
     - sales-network
  sales-core:
    image: cuba-sample-sales-core
    networks:
     - sales-network
  sales-web:
    image: cuba-sample-sales-web
    ports:
     - "8080:8080"
    networks:
     - sales-network

networks:
  sales-network:
----

Соберите образы с помощью следующих команд:

[source, plain]
----
docker build -t cuba-sample-sales-web ./web
----

[source, plain]
----
docker build -t cuba-sample-sales-core ./core
----

Для запуска приложения перейдите в директорию с  файлом `docker-compose.yml` и запустите следующую команду:

[source, plain]
----
docker-compose up
----

Приложение будет доступно по адресу `++http://localhost:8080/app++`.

[TIP]
=====
Для развертывания контейнеров на нескольких физических машинам вам может понадобиться установить и настроить https://docs.docker.com/engine/swarm/key-concepts/[Docker Swarm] или https://kubernetes.io/[Kubernetes].
=====

===== Плагин Gradle для Docker

В этом разделе на примере монолитной конфигурации Uber JAR показано как с помощью плагина Gradle для Docker создать и опубликовать образ приложения CUBA.

Для сборки образа Docker из Gradle можно использовать плагин: https://github.com/bmuschko/gradle-docker-plugin[bmuschko/gradle-docker-plugin].

Для использования плагина в файл `build.gradle` нужно добавить зависимости и импортировать необходимые классы для работы с образами как показано в примере (X.Y.Z нужно заменить на актуальную версию плагина):

[source, plain]
----
buildscript {

    dependencies {
        classpath 'com.bmuschko:gradle-docker-plugin:X.Y.Z'
    }
}

import com.bmuschko.gradle.docker.tasks.image.Dockerfile
import com.bmuschko.gradle.docker.tasks.image.DockerBuildImage
import com.bmuschko.gradle.docker.tasks.image.DockerPushImage
import com.bmuschko.gradle.docker.DockerRegistryCredentials
----

Плагин `com.bmuschko.docker-remote-api` дает возможность взаимодействовать с Docker при помощи удаленного API. Можно смоделировать любой рабочий процесс, создав свою задач на основе пользовательской задачи, предоставляемой плагином.
Для того, чтобы воспользоваться плагином, добавьте следующий фрагмент кода в файл `build.gradle`:

[source, plain]
----
apply plugin: 'com.bmuschko.docker-remote-api'
----

Dockerfile можно создать с помощью  пользовательской задачи `Dockerfile`. Инструкции для `Dockerfile` должны соответствовать определенной структуре. Инструкции в файле обрабатываются сверху вниз. Каждая инструкция добавляет новый слой в образ и фиксирует изменения. Docker исполняет инструкции, следуя процессу:

* Запуск контейнера из образа.
* Исполнение инструкции и внесение изменений в контейнер.
* Запуск эквивалента docker commit для записи изменений в новый слой образа.
* Запуск нового контейнера из нового образа.
* Исполнение следующей инструкции в файле и повторение шагов процесса.

Задача создания `Dockerfile` будет выглядеть следующим образом:

[source, groovy]
----
task createDockerfile(type: Dockerfile, dependsOn: buildUberJar)  {
    destFile = project.file('build/distributions/uberJar/Dockerfile')
    from 'openjdk:8'
    addFile("app.jar", "/usr/src/cuba-sales/app.jar")
    defaultCommand("java", "-Dapp.home=/usr/src/cuba-sales/home", "-jar", "/usr/src/cuba-sales/app.jar")
}
----

* Свойство `from` устанавливает базовый образ для `Dockerfile`, последующие инструкции выполняют построение поверх данного образа.
* Свойство `addFile` определяет путь до JAR-файла, который будет скопирован в образ. Следует заметить, что JAR-файл должен быть размещен в одной папке с `Dockerfile`.
* Свойство `defaultCommand` определяет набор инструкций, который будет выполнен при запуске контейнера.

Операции скачивания и публикации образов в публичный репозиторий Docker Hub или закрытый репозиторий могут потребовать аутентификации. Учетные данные можно передать в метод с помощью объекта registryCredentials. Определите свои учетные данные в файле https://docs.gradle.org/current/userguide/build_environment.html[gradle.properties]:

[source, groovy]
----
dockerHubEmail = 'example@email.com'
dockerHubPassword = 'docker-hub-password'
dockerHubUsername = 'docker-hub-username'
----

После этого можно получить доступ к этим свойствам в файле `build.gradle` по имени:

[source, groovy]
----
def dockerRegistryCredentials = new DockerRegistryCredentials()
dockerRegistryCredentials.email = dockerHubEmail
dockerRegistryCredentials.password = dockerHubPassword
dockerRegistryCredentials.username = dockerHubUsername
----

Для создания образа приложения CUBA с помощью с `Dockerfile` и публикации этого образа в публичный репозиторий Docker Hub нужно определить следующие задачи:

[source, groovy]
----
include::{sourcesdir}/deployment/dockerFile.groovy[]
----

Настройте и соберите монолитный Uber JAR как показано в разделе <<single_jar_deployment>>. После этого запустите задачу `pushImage` из терминала или из поля *Search* в Студии.

[source, plain]
----
gradle pushImage
----

Эта задача последовательно собирает Uber JAR, генерирует `Dockerfile` с необходимыми инструкцими, создает образ и публикует его в репозиторий Docker Hub.

===== Развертывание контейнера в Heroku

Настройте и соберите монолитный Uber JAR как показано в разделе <<single_jar_deployment>>. Создайте аккаунт в  Heroku и установите Heroku CLI. Более детально эти действия описаны в разделе <<heroku_war_deployment>>.

Создайте приложение с уникальным именем и подключите к нему базу данных (в примере подключается PostgreSQL с бесплатным тарифным планом  hobby-dev) с помощью Heroku CLI:

[source, plain]
----
heroku create cuba-sales-docker --addons heroku-postgresql:hobby-dev
----

После создания базы данных нужно указать детали для подключения к сервису БД в файле `jetty-env.xml`.

. Откройте https://dashboard.heroku.com.
. Выберите проект, откройте вкладку *Resources* и выберите базу данных.
. В новом окне перейдите на вкладку *Settings* и нажмите на кнопку *View Credentials*.

image::heroku-db.png[Db,1200,1000]

[[jetty-env]]
Откройте проект в IDE, откройте файл `jetty-env.xml`. Необходимо поменять URL (имя хоста и базы данных), имя пользователя и пароль. Для этого нужно скопировать данные, указанные на сайте, в файл.

[source, xml]
----
include::{sourcesdir}/deployment/jetty-env.xml[]
----

Соберите монолитный Uber JAR-файл с помощью команды Gradle:

[source, plain]
----
gradle buldUberJar
----

Также необходимо внести изменения в `Dockerfile`. Прежде всего нужно ограничить объем памяти, потребляемой приложением. Затем нужно получить порт приложения из Heroku и добавить его к образу.

Файл `Dockerfile` будет выглядеть следующим образом:

[source, plain]
----
### Dockerfile

FROM openjdk:8

COPY . /usr/src/cuba-sales

CMD java -Xmx512m -Dapp.home=/usr/src/cuba-sales/home -jar /usr/src/cuba-sales/app.jar -port $PORT
----

Откройте командную строку в папке `$PROJECT_FOLDER` и запустите команды, указанные ниже:

[source, plain]
----
git init
heroku git:remote -a cuba-sales-docker
git add .
git commit -am "Initial commit"
----

После этого нужно зайти в репозиторий контейнеров, это место для хранения образов в Heroku.

[source, plain]
----
heroku container:login
----

Теперь можно создать образ и загрузить его в репозиторий контейнеров.

[source, plain]
----
heroku container:push web
----

Здесь `web` -- тип процесса приложения. При запуске этой команды Heroku по умолчанию создает образ с помощью `Dockerfile` в текущем каталоге, а затем загружает его в Heroku.

После завершения процесса развертывания ваше приложение будет доступно в браузере по ссылке https://cuba-sales-docker.herokuapp.com/app

Вы также можете открыть приложение с помощью кнопки *Open app*, расположенной на панели Heroku.

Третий способ открыть запущенное приложение -- использовать следующую команду (необходимо добавить `app` контекст к ссылке, т.е. https://cuba-sales-docker.herokuapp.com/app):

[source, plain]
----
heroku open
----

[[proxy_configuration]]
=== Конфигурация прокси для Tomcat

Для задач интеграции может потребоваться прокси-сервер. В этом разделе описывается конфигурация HTTP-сервера Nginx в качестве прокси для приложения на платформе CUBA.

[TIP]
====
Если вы настраиваете прокси, то не забудьте задать значение в параметре <<cuba.webAppUrl,cuba.webAppUrl>>.
====

NGINX::

Для Nginx предлагается 2 конфигурации проксирования, описанные ниже. Все примеры подготовлены и проверены на Ubuntu 16.04.

. <<direct_proxy,Прямое проксирование>>
. <<redirect_to_path,Проксирование с перенаправлением>>

К примеру, ваше веб-приложение работает по ссылке `++http://localhost:8080/app++`.

[TIP]
====
Кроме Nginx следует настроить еще и <<tomcat_for_proxy,Tomcat>>.
====

[[tomcat_for_proxy]]
Настройка Tomcat::
+
--
Сначала добавьте в конфигурационный файл Tomcat `conf/server.xml` следующий код:

[source,xml]
----
<Valve className="org.apache.catalina.valves.RemoteIpValve"
        remoteIpHeader="X-Forwarded-For"
        requestAttributesEnabled="true"
        internalProxies="127\.0\.0\.1"  />
----

и перезапустите Tomcat:

[source,plain]
----
sudo service tomcat restart
----

Это требуется для того, чтобы Tomcat мог обрабатывать заголовки от Nginx без модификации веб-приложения.

[[install_nginx]]
Затем установите Nginx:

[source,plain]
----
sudo apt-get install nginx
----

Откройте в браузере ссылку `++http://localhost++` и убедитесь, что стартовая страница Nginx работает.

Теперь вы можете удалить символьную ссылку на тестовый сайт Nginx:

[source,plain]
----
rm /etc/nginx/sites-enabled/default
----
--

Далее сконфигурируйте прокси одной из выбранных схем:

[[direct_proxy]]
Прямое проксирование::
+
--
В этом случае все запросы обрабатывает прокси, прозрачно перенаправляя их в приложение.

Создайте конфигурационный файл Nginx `/etc/nginx/sites-enabled/direct_proxy`:

[source,plain]
----
include::{sourcesdir}/deployment/direct_proxy.yml[]
----

и перезапустите Nginx:

[source,plain]
----
sudo service nginx restart
----

Вы можете открыть свой сайт по ссылке `++http://localhost/app++`.
--

[[redirect_to_path]]
Проксирование с перенаправлением::
+
--
В этом примере описано, как изменить путь к приложению в URL с `/app` на `/`, как если бы приложение было развёрнуто в корневом контексте (аналог `/ROOT`). Это позволит вам обращаться к приложению по адресу `++http://localhost++`.

Создайте конфигурационный файл Nginx `/etc/nginx/sites-enabled/direct_proxy`:

[source,plain]
----
include::{sourcesdir}/deployment/root_proxy[]
----

и перезапустите Nginx

[source,plain]
----
sudo service nginx restart
----

Ваше приложение доступно по ссылке `++http://localhost++`.
--

[TIP]
====
Обратите внимание, что аналогичные инструкции развертывания прокси справедливы для конфигураций <<proxy_configuration_uberjar,Jetty>>, WildFly и др. В таких случаях вам может понадобиться дополнительная конфигурация этих серверов.
====

[[proxy_configuration_uberjar]]
=== Конфигурация прокси для Uber JAR

В этой части рассказывается, как настроить HTTP-сервер Nginx в качестве прокси для приложения CUBA Uber JAR.

NGINX::
--
Для Nginx предлагается 2 конфигурации проксирования, описанных ниже. Все примеры подготовлены и проверены на Ubuntu 16.04.

. Прямое проксирование
. Проксирование с перенаправлением

К примеру, ваше веб-приложение работает по ссылке `++http://localhost:8080/app++`.

[TIP]
====
Приложение Uber JAR использует сервер Jetty версии 9.2. Jetty внутри <<proxy_for_jetty,JAR>> следует сконфигурировать таким образом, чтобы он обрабатывал заголовки Nginx.
====
--

[[proxy_for_jetty]]
Настройка Jetty::
+
--
* *Настройка внутри JAR*
+
Сначала создайте конфигурационный файл `jetty.xml` в корне проекта и вставьте в него следующий код:
+
[source,xml]
----
include::{sourcesdir}/deployment/jetty.xml[]
----
+
Добавьте свойство `webJettyConfPath` в задачу `buildUberJar` вашего файла `build.gradle`:
+
[source,groovy]
----
task buildUberJar(type: CubaUberJarBuilding) {
    singleJar = true
    coreJettyEnvPath = 'modules/core/web/META-INF/jetty-env.xml'
    appProperties = ['cuba.automaticDatabaseUpdate' : true]
    webJettyConfPath = 'jetty.xml'
}
----
+
Соберите Uber JAR, используя следующую команду:
+
[source,plain]
----
gradlew buildUberJar
----
+
Ваше приложение будет расположено в папке `build/distributions/uberJar`, имя по-умолчанию: `app.jar`.
+
Запустите приложение:
+
[source,plain]
----
java -jar app.jar
----
+
Затем установите и настройте Nginx, как описано в <<install_nginx,секции Tomcat>>.
+
В зависимости от выбранной схемы проксирования, ваш сайт будет доступен по одной из ссылок: `++http://localhost/app++` или `++http://localhost++`.

* *Настройка с помощью внешнего файла*
+
Используйте тот же самый конфигурационный файл `jetty.xml` в корне проекта, как описано выше, но не изменяйте `build.gradle`.
+
Соберите Uber JAR, используя следующую команду:
+
[source,plain]
----
gradlew buildUberJar
----
+
Ваше приложение будет расположено в папке `build/distributions/uberJar`, имя по-умолчанию: `app.jar`.
+
Запустите приложение с параметром `-jettyConfPath`:
+
[source,plain]
----
java -jar app.jar -jettyConfPath jetty.xml
----
+
Затем установите и настройте Nginx, как описано в <<install_nginx,секции Tomcat>>.
+
В зависимости от выбранной схемы проксирования и настроек в `jetty.xml`, ваш сайт будет доступен по одной из ссылок: `++http://localhost/app++` или `++http://localhost++`.
--

[[scaling]]
=== Масштабирование приложения

В данном разделе рассмотрены способы масштабирования CUBA-приложения, состоящего из блоков Middleware и Web Client, при возрастании нагрузки и ужесточении требований к отказоустойчивости.

[cols="3,2", frame="all", width="100%"]
|===

a| *Этап 1. Оба блока развернуты на одном сервере приложения.*

Это простейший вариант, реализуемый стандартной процедурой <<fast_deployment,быстрого развертывания>>.

В данном случае обеспечивается максимальная производительность передачи данных между блоками *Web Client* и *Middleware*, так как при включенном свойстве приложения <<cuba.useLocalServiceInvocation,cuba.useLocalServiceInvocation>> сервисы Middleware вызываются в обход сетевого стека.
^| image:scaling_1.png[align="center"]

a| *Этап 2. Блоки Middleware и Web Client развернуты на отдельных серверах приложения.*

Данный вариант позволяет распределить нагрузку между двумя серверами приложения и более оптимально использовать ресурсы серверов. Кроме того, в этом случае нагрузка от веб-пользователей меньше сказывается на выполнении других процессов. Под другими процессами здесь понимается обслуживание средним слоем других типов клиентов (например Desktop), выполнение <<scheduled_tasks,задач по расписанию>> и, возможно, интеграционные задачи.

Требования к ресурсам серверов:

* Tomcat 1 (Web Client):
** Объем памяти - пропорционально количеству одновременно подключенных пользователей.
** Мощность CPU - зависит от интенсивности работы пользователей.
* Tomcat 2 (Middleware):
** Объем памяти - фиксированный и относительно небольшой.
** Мощность CPU - зависит от интенсивности работы пользователей и других процессов.

В этом и более сложных вариантах развертывания в блоке Web Client свойство приложения <<cuba.useLocalServiceInvocation,cuba.useLocalServiceInvocation>> должно быть установлено в false, а свойство <<cuba.connectionUrlList,cuba.connectionUrlList>> должно содержать URL блока Middleware.
^| image:scaling_2.png[align="center"]

| *Этап 3. Кластер серверов Web Client работает с одним сервером Middleware.*

Данный вариант применяется, когда вследствие большого количества одновременно подключенных пользователей требования к памяти для блока Web Client превышают возможности одной JVM. В этом случае запускается кластер (два или более) серверов Web Client, и подключение пользователей производится через Load Balancer. Все серверы Web Client работают с одним сервером Middleware.

Дублирование серверов Web Client автоматически обеспечивает отказоустойчивость на этом уровне. Однако, так как репликация HTTP-сессий не поддерживается, при незапланированном отключении одного из серверов Web Client все пользователи, подключенные к нему, вынуждены будут выполнить новый логин в приложение.

Настройка данного варианта развертывания описана в <<cluster_webclient,Настройка кластера Web Client>>.
^| image:scaling_3.png[align="center"]

| *Этап 4. Кластер серверов Web Client работает с кластером серверов Middleware.*

Это максимальный вариант развертывания, обеспечивающий отказоустойчивость и балансировку нагрузки для Middleware и Web Client.

Подключение пользователей к серверам Web Client производится через Load Balancer. Серверы WebClient работают с кластером серверов Middleware. Для этого им не требуется дополнительный Load Balancer - достаточно определить список URL серверов Middleware в свойстве <<cuba.connectionUrlList,cuba.connectionUrlList>>. Можно также использовать <<cluster_mw_zk,дополнение для интеграции с Apache ZooKeeper>> для динамического обнаружения серверов среднего слоя.

В кластере серверов Middleware организуется взаимодействие для обмена информацией о пользовательских сессиях, блокировках и пр. При этом обеспечивается полная отказоустойчивость блока Middleware - при отключении одного из серверов выполнение запросов от клиентских блоков продолжается на доступном сервере прозрачно для пользователей.

Настройка данного варианта развертывания описана в <<cluster_mw,Настройка кластера Middleware>>.
^| image:scaling_4.png[align="center"]

|===

[[cluster_webclient]]
==== Настройка кластера Web Client

В данном разделе рассматривается следующая конфигурация развертывания:

image::cluster_webclient.png[align="center"]

Здесь на серверах `host1` и `host2` блок установлены инстансы Tomcat с веб-приложением `app`, реализующим блок Web Client. Пользователи обращаются к балансировщику нагрузки по адресу `http://host0/app`, который перенаправляет запрос этим серверам. На сервере `host3` установлен Tomcat с веб-приложением `app-core`, реализующим блок Middleware.

[[cluster_webclient_lb]]
===== Установка и настройка Load Balancer

Рассмотрим процесс установки балансировщика нагрузки на базе *Apache HTTP Server* для операционной системы *Ubuntu 14.04*.

. Выполните установку *Apache HTTP Server* и его модуля *mod_jk*:
+
`$ sudo apt-get install apache2 libapache2-mod-jk`

. Замените содержимое файла `/etc/libapache2-mod-jk/workers.properties` на следующее:
+
[source, plain]
----
workers.tomcat_home=
workers.java_home=
ps=/

worker.list=tomcat1,tomcat2,loadbalancer,jkstatus

worker.tomcat1.port=8009
worker.tomcat1.host=host1
worker.tomcat1.type=ajp13
worker.tomcat1.connection_pool_timeout=600
worker.tomcat1.lbfactor=1

worker.tomcat2.port=8009
worker.tomcat2.host=host2
worker.tomcat2.type=ajp13
worker.tomcat2.connection_pool_timeout=600
worker.tomcat2.lbfactor=1

worker.loadbalancer.type=lb
worker.loadbalancer.balance_workers=tomcat1,tomcat2

worker.jkstatus.type=status
----

. Добавьте в файл `/etc/apache2/sites-available/000-default.conf` следующее:
+
[source, xml]
----
<VirtualHost *:80>
...
    <Location /jkmanager>
        JkMount jkstatus
        Order deny,allow
        Allow from all
    </Location>

    JkMount /jkmanager/* jkstatus
    JkMount /app loadbalancer
    JkMount /app/* loadbalancer

</VirtualHost>
----

. Перезапустите сервис Apache HTTP:
+
`$ sudo service apache2 restart`


[[cluster_webclient_tomcat]]
===== Настройка серверов Web Client

[TIP]
====
В примерах ниже пути к конфигурационным файлам приводятся для варианта <<fast_deployment>>.
====

На серверах Tomcat 1 и Tomcat 2 необходимо произвести следующие настройки:

. В файлах `tomcat/conf/server.xml` добавить параметр `jvmRoute`, эквивалентный имени worker, заданному в настройках балансировщика нагрузки - `tomcat1` и `tomcat2`:
+
[source, xml]
----
<Server port="8005" shutdown="SHUTDOWN">
  ...
  <Service name="Catalina">
    ...
    <Engine name="Catalina" defaultHost="localhost" jvmRoute="tomcat1">
      ...
    </Engine>
  </Service>
</Server>
----

. Задать следующие свойства приложения в файлах `tomcat/conf/app/local.app.properties`:
+
[source, plain]
----
cuba.useLocalServiceInvocation = false
cuba.connectionUrlList = http://host3:8080/app-core

cuba.webHostName = host1
cuba.webPort = 8080
cuba.webContextName = app
----
+
Параметры <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>>, <<cuba.webContextName,cuba.webContextName>> не обязательны для работы кластера WebClient, но позволяют проще идентифицировать сервера в других механизмах платформы, например в <<jmx_console,консоли JMX>>. Кроме того, в экране *User Sessions* в атрибуте *Client Info* отображается сформированный из этих параметров идентификатор блока Web Client, на котором работает данный пользователь.

[[cluster_mw]]
==== Настройка кластера Middleware

В данном разделе рассматривается следующая конфигурация развертывания:

image::cluster_mw.png[align="center"]

Здесь на серверах `host1` и `host2` блок установлены инстансы Tomcat с веб-приложением `app`, реализующим блок Web Client. Настройка кластера этих серверов рассмотрена в <<cluster_webclient,предыдущем разделе>>. На серверах `host3` и `host4` установлены инстансы Tomcat с веб-приложением `app-core`, реализующим блок Middleware. Между ними настроено взаимодействие для обмена информацией о пользовательских сессиях и блокировках, сброса кэшей и др.

[TIP]
====
В примерах ниже пути к конфигурационным файлам приводятся для варианта <<fast_deployment>>.
====

[[cluster_mw_client]]
===== Настройка обращения к кластеру Middleware

Для того, чтобы клиентские блоки могли работать с несколькими серверами Middleware, достаточно указать список URL этих серверов в свойстве приложения <<cuba.connectionUrlList,cuba.connectionUrlList>>. Для Web Client это можно сделать в файле `tomcat/conf/app/local.app.properties`:

[source, plain]
----
cuba.useLocalServiceInvocation = false
cuba.connectionUrlList = http://host3:8080/app-core,http://host4:8080/app-core

cuba.webHostName = host1
cuba.webPort = 8080
cuba.webContextName = app
----

Сервер среднего слоя выбирается в случайном порядке в момент первого обращения для данной <<userSession,пользовательской сессии>>, и фиксируется на все время жизни сессии ("sticky session"). Запросы от анонимной сессии и без сессии не фиксируются и выполняются на серверах выбираемых в случайном порядке.

Алгоритм выбора сервера предоставляется бином `cuba_ServerSorter`, который по умолчанию реализован классом `RandomServerSorter`. В проекте можно реализовать собственный алгоритм выбора.

[[cluster_mw_server]]
===== Настройка взаимодействия серверов Middleware

Сервера Middleware могут поддерживать общие списки <<userSession,пользовательских сессий>> и других объектов, а также координировать сброс кэшей. Для этого достаточно на каждом их них включить свойство приложения <<cuba.cluster.enabled,cuba.cluster.enabled>>. Пример файла `tomcat/conf/app-core/local.app.properties`:

[source, plain]
----
cuba.cluster.enabled = true

cuba.webHostName = host3
cuba.webPort = 8080
cuba.webContextName = app-core
----

Для серверов Middleware обязательно нужно указать правильные значения свойств <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>> и <<cuba.webContextName,cuba.webContextName>> для формирования уникального <<serverId,Server Id>>.

Механизм взаимодействия основан на библиотеке link:$$http://www.jgroups.org$$[JGroups]. Платформа содержит два конфигурационных файла для JGroups:

* `jgroups.xml` - стек протоколов основанный на UDP, пригодный для работы в локальной сети с разрешенными широковещательными сообщениями. Данная конфигурация используется по умолчанию.

* `jgroups_tcp.xml` - стек протоколов основанный на TCP, пригодный для работы в любой сети. Он требует явного указания адресов узлов кластера в параметрах `TCP.bind_addr` и `TCPPING.initial_hosts`. Для использования данной конфигурации настройте свойство приложения <<cuba.cluster.jgroupsConfig,cuba.cluster.jgroupsConfig>>.

Для настройки параметров JGroups для вашего окружения скопируйте подходящий файл `jgroups.xml` из корня архива `cuba-core-<version>.jar` в модуль *core* вашего проекта или в каталог `tomcat/conf/app-core`, и настройте его нужным образом.

Программный интерфейс для взаимодействия в кластере Middleware обеспечивает бин `ClusterManagerAPI`. Его можно использовать в приложении - см. JavaDocs и примеры использования в коде платформы.

[[cluster_mw_zk]]
===== Использование ZooKeeper для координации кластера

Существует <<app_components,компонент приложения>>, обеспечивающий динамическое обнаружение серверов middleware для коммуникации между блоками middleware и для запросов с клиентских блоков. Он основан на интеграции с https://zookeeper.apache.org[Apache ZooKeeper] - централизованным сервисом для работы с конфигурационной информацией. Если данный компонент включен в проект, при запуске блоков приложения необходимо указывать только один статический адрес - адрес ZooKeeper. При этом сервера middleware публикуют свои адреса в каталоге ZooKeeper, а механизм обнаружения запрашивает ZooKeeper для получения адресов доступных серверов. Если сервер middleware останавливается, его адрес автоматически исключается из каталога (немедленно или по истечении таймаута).

Исходный код компонента доступен на https://github.com/cuba-platform/cuba-zk[GitHub], бинарные артефакты опубликованы в репозиториях платформы. См. https://github.com/cuba-platform/cuba-zk[README] для получения информации по установке и использованию компонента.

[[serverId]]
==== Server Id

_Server Id_ служит для надежной идентификации серверов в кластере *Middleware*. Идентификатор имеет вид `host:port/context`, например:

[source, plain]
----
tezis.haulmont.com:80/app-core
----

[source, plain]
----
192.168.44.55:8080/app-core
----

Идентификатор формируется на основе параметров конфигурации <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>>, <<cuba.webContextName,cuba.webContextName>>, поэтому крайне важно корректно указать эти параметры для блока *Middleware*, работающего в кластере.

Server Id может быть получен c помощью бина `ServerInfoAPI` или через JMX-интерфейс `<<serverInfoMBean,ServerInfoMBean>>`.

[[jmx_tools]]
=== Использование инструментов JMX

В данном разделе рассмотрены различные аспекты использования инструментов *Java Management Extensions* в CUBA-приложениях.

[[jmx_console]]
==== Встроенная JMX консоль

Модуль *Web Client* базового проекта *cuba* платформы содержит средство просмотра и редактирования JMX объектов. Точкой входа в этот инструмент является экран `com/haulmont/cuba/web/app/ui/jmxcontrol/browse/display-mbeans.xml`, зарегистрированный под идентификатором `jmxConsole` и в стандартном меню доступный через пункт *Администрирование* → *Консоль JMX*.

Без дополнительной настройки консоль отображает все JMX объекты, зарегистрированные в JVM, на которой работает блок *Web Client*, к которому в данный момент подключен пользователь. Соответственно, в простейшем случае развертывания всех блоков приложения в одном экземпляре веб-контейнера консоль имеет доступ к JMX бинам всех уровней, а также к JMX объектам самой JVM и веб-контейнера. 

Имена бинов приложения имеют префикс, соответствующий имени веб-приложения, их содержащего. Например, бин `app-core.cuba:type=CachingFacade` загружен веб-приложением *app-core*, реализующим блок *Middleware*, а бин `app.cuba:type=CachingFacade` загружен веб-приложением *app*, реализующим блок *Web Client*.

.JMX консоль
image::jmx-console.png[align="center"]

Консоль JMX может также работать с JMX объектами произвольной удаленной JVM. Это актуально в случае развертывания блоков приложения на нескольких экземплярах веб-контейнера, например, отдельно *Web Client* и *Middleware*. 

Для подключения к удаленной JVM необходимо в поле *Соединение JMX* консоли выбрать созданное ранее соединение, либо вызвать экран создания нового соединения:

.Редактирование JMX соединения
image::jmx-connection-edit.png[align="center"]

Для соединения указывается JMX хост и порт, логин и пароль. Имеется также поле *Имя узла*, которое заполняется автоматически, если по указанному адресу обнаружен какой-либо блок CUBA-приложения. В этом случае значением этого поля становится комбинация свойств `<<cuba.webHostName,cuba.webHostName>>` и `<<cuba.webPort,cuba.webPort>>` данного блока, что позволяет идентифицировать содержащий его сервер. Если подключение произведено к постороннему JMX интерфейсу, то поле *Имя узла* будет иметь значение "Unknown JMX interface". Значение данного поля можно произвольно изменять.

Для подключения удаленной JVM она должна быть соответствующим образом настроена - см. ниже.

[[jmx_remote_access]]
==== Настройка удаленного доступа к JMX

В данном разделе рассматривается настройка запуска сервера *Tomcat*, необходимая для удаленного подключения к нему инструментов JMX.

[[jmx_remote_access_tomcat_windows]]
===== Tomcat JMX под Windows

* Отредактировать файл `bin/setenv.bat` следующим образом:
+
[source, plain]
----
set CATALINA_OPTS=%CATALINA_OPTS% ^
-Dcom.sun.management.jmxremote ^
-Djava.rmi.server.hostname=192.168.10.10 ^
-Dcom.sun.management.jmxremote.ssl=false ^
-Dcom.sun.management.jmxremote.port=7777 ^
-Dcom.sun.management.jmxremote.authenticate=true ^
-Dcom.sun.management.jmxremote.password.file=../conf/jmxremote.password ^
-Dcom.sun.management.jmxremote.access.file=../conf/jmxremote.access
----
+
Здесь в параметре `java.rmi.server.hostname` необходимо указать реальный IP адрес или DNS имя компьютера, на котором запущен сервер, в параметре `com.sun.management.jmxremote.port` - порт для подключения инструментов JMX.

* Отредактировать файл `conf/jmxremote.access`. Он должен содержать имена пользователей, которые будут подключаться к JMX, и их уровень доступа. Например:
+
[source, plain]
----
admin readwrite
----

* Отредактировать файл `conf/jmxremote.password`. Он должен содержать пароли пользователей JMX, например:
+
[source, plain]
----
admin admin
----

* Файл паролей должен иметь разрешение на чтение только для пользователя, от имени которого работает сервер *Tomcat*. Настроить права можно следующим образом:

** Открыть командную строку и перейти в каталог `conf`.

** Выполнить команду:
+
`++cacls jmxremote.password /P "domain_name\user_name":R++`
+
где `++domain_name\user_name++` - домен и имя пользователя.

** После выполнения данной команды файл в *Проводнике* будет отмечен изображением замка.

* Если *Tomcat* установлен как служба Windows, то для службы должен быть задан вход в систему с учетной записью, имеющей права на файл `jmxremote.password`. Кроме того, следует иметь в виду, что в этом случае файл `bin/setenv.bat` не используется, и соответствующие параметры запуска JVM должны быть заданы в приложении, настраивающем службу.

[[jmx_remote_access_tomcat_linux]]
===== Tomcat JMX под Linux

* Отредактировать файл `bin/setenv.sh` следующим образом:
+
[source,plain]
----
CATALINA_OPTS="$CATALINA_OPTS -Dcom.sun.management.jmxremote \
-Djava.rmi.server.hostname=192.168.10.10 \
-Dcom.sun.management.jmxremote.port=7777 \
-Dcom.sun.management.jmxremote.ssl=false \
-Dcom.sun.management.jmxremote.authenticate=true"

CATALINA_OPTS="$CATALINA_OPTS -Dcom.sun.management.jmxremote.password.file=../conf/jmxremote.password -Dcom.sun.management.jmxremote.access.file=../conf/jmxremote.access"
----
+
Здесь в параметре `java.rmi.server.hostname` необходимо указать реальный IP адрес или DNS имя компьютера, на котором запущен сервер, в параметре `com.sun.management.jmxremote.port` - порт для подключения инструментов JMX.

* Отредактировать файл `conf/jmxremote.access`. Он должен содержать имена пользователей, которые будут подключаться к JMX, и их уровень доступа. Например:
+
[source, plain]
----
admin readwrite
----

* Отредактировать файл `conf/jmxremote.password`. Он должен содержать пароли пользователей JMX, например:
+
[source, plain]
----
admin admin
----

* Файл паролей должен иметь разрешение на чтение только для пользователя, от имени которого работает сервер *Tomcat*. Настроить права для текущего пользователя можно следующим образом:

** Открыть командную строку и перейти в каталог `conf`.

** Выполнить команду:
+
`chmod go-rwx jmxremote.password`

[[server_push_settings]]
=== Настройка server push

Приложения CUBA используют технологию server push в механизме <<background_tasks,фоновых задач>>. Это может потребовать дополнительной настройки сервера приложения и прокси-сервера (если таковой используется).

По умолчанию server push использует протокол WebSocket. Следующие свойства приложения влияют на функциональность server push платформы:

<<cuba.web.pushLongPolling,cuba.web.pushLongPolling>>

<<cuba.web.pushEnabled,cuba.web.pushEnabled>>

Информация ниже взята из статьи с веб-сайта Vaadin - https://vaadin.com/docs/v8/framework/articles/ConfiguringPushForYourEnvironment.html[Configuring push for your environment].

[[server_push_settings_err_incomplete_chunked_encoding]]
Chrome показывает ERR_INCOMPLETE_CHUNKED_ENCODING::
+
--
Это совершенно нормально и означает, что (long-polling) push-соединение было прервано третьей стороной. Обычно это происходит, когда между браузером и сервером имеется прокси-сервер, а прокси-сервер имеет настроенный тайм-аут и отключает соединение, когда тайм-аут достигнут. После этого браузер должен повторно подключиться к серверу.
--

[[server_push_settings_]]
Tomcat 8 + Websockets::
+
--
[source, plain]
----
java.lang.ClassNotFoundException: org.eclipse.jetty.websocket.WebSocketFactory$Acceptor
----

Это означает, что где то в classpath развернут Jetty. Atmosphere путается и использует реализацию Websocket от Jetty сервера вместо Tomcat. Одной из распространенных причин этого является то, что Вы случайно развернули `vaadin-client-compiler`, у которого Jetty есть в качестве зависимости (требуется, например, для SuperDevMode).
--

[[server_push_settings_glassfish_streaming]]
Glassfish 4 + Streaming::
+
--
Чтобы Streaming режим работал в Glassfish 4, необходимо включить опцию *comet*.


Для этого установите

[source, plain]
----
(Configurations → server-config → Network Config → Protocols → http-listener-1 → HTTP → Comet Support)
----

или используйте

[source, plain]
----
asadmin set server-config.network-config.protocols.protocol.http-listener-1.http.comet-support-enabled="true"
----
--

[[server_push_settings_glassfish_websockets]]
Glassfish 4 + Websockets::
+
--
Если Вы используете Glassfish 4.0, то для избежания проблем Вам необходимо обновиться на версию Glassfish 4.1.
--

[[server_push_settings_weblogic_websockets]]
Weblogic 12 + Websockets::
+
--
Используйте WebLogic 12.1.3 или новее. По умолчанию, WebLogic 12 имеет 30 сек. таймаут для websocket соединений. Чтобы избежать постоянных повторных соединений, вы можете установить параметр `weblogic.websocket.tyrus.session-max-idle-timeout` либо в значение `-1` (т.е. без таймаута), либо в большее чем `30000` (значение в миллисекундах).
--

[[server_push_settings_jboss_websockets]]
JBoss EAP 6.4 + Websockets::
+
--
JBoss EAP 6.4 включает поддержку websockets, но по умолчанию они отключены. Чтобы включить websockets, Вам необходимо переключить JBoss в режим использования NIO-коннектора, запустив:

[source, plain]
----
$ bin/jboss-cli.sh --connect
----

затем выполнив следующие команды:

[source, plain]
----
batch
/subsystem=web/connector=http/:write-attribute(name=protocol,value=org.apache.coyote.http11.Http11NioProtocol)
run-batch
:reload
----

Чтобы включить websockets, необходимо добавить *WEB-INF/jboss-web.xml* в Ваш war-файл со следующим содержанием:

[source, xml]
----
<jboss-web version="7.2" xmlns="http://www.jboss.com/xml/ns/javaee"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://www.jboss.com/xml/ns/javaee schema/jboss-web_7_2.xsd">
    <enable-websockets>true</enable-websockets>
</jboss-web>
----
--

[[server_push_settings_duplicate_resource]]
Duplicate resource::
+
--
Если логи сервера содержат

[source, plain]
----
Duplicate resource xyz-abc-def-ghi-jkl. Could be caused by a dead connection not detected by your server. Replacing the old one with the fresh one
----

Это указывает на то, что сначала браузер подключился к серверу и использовал данный идентификатор для push-соединения. Позже браузер (возможно, тот же самый) снова подключился с использованием того же идентификатора, но сервер считает, что старое соединение с браузером все еще активно. Сервер закрывает старое соединение и регистрирует предупреждение.

Это происходит из-за того, что между браузером и сервером существует прокси-сервер, а прокси-сервер настроен так, чтобы убивать открытые соединения после определенного таймаута бездействия (данные не отправляются до того, как сервер выдает команду push). Из-за того, как работает TCP/IP, сервер не знает, что соединение было убито и продолжает думать, что старый клиент подключен, и все в порядке.

У вас есть несколько вариантов, чтобы избежать этой проблемы:

. Если вы контролируете прокси-сервер, настройте его не закрывать push-соединения (подключения к URL-адресу `/PUSH`).
. Если Вы знаете какой таймаут задан на прокси-сервере, установить таймаут для push в приложении немного меньше. Тогда сервер будет завершать неактивное соединение до того, как сработает таймаут прокси-сервера.
.. Установить параметр `cuba.web.pushLongPolling` в значение `true` чтобы включить long polling вместо websocket.
.. Используйте параметр `cuba.web.pushLongPollingSuspendTimeoutMs`, чтобы установить таймаут push в миллисекундах.
--

[[server_push_settings_using_proxy]]
Использование Proxy::
+
--
Если пользователи подключаются к серверу приложения через прокси, не поддерживающий WebSocket, рекомендуется установить свойство `cuba.web.pushLongPolling` в `true` и увеличить таймаут запроса на прокси до 10 минут или больше.

Ниже приведен пример конфигурации веб-сервера *Nginx* для использования WebSocket:

[source, plain]
----
location / {
    proxy_set_header X-Forwarded-Host $host;
    proxy_set_header X-Forwarded-Server $host;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_read_timeout     3600;
    proxy_connect_timeout  240;
    proxy_set_header Host $host;
    proxy_set_header X-RealIP $remote_addr;

    proxy_pass http://127.0.0.1:8080/;
    proxy_set_header X-Forwarded-Proto $scheme;

    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection "upgrade";
}
----
--

[[health_check_url]]
=== Health check URL

Каждый блок приложения, развернутый как веб-приложение, предоставляет URL для проверки своего состояния. HTTP GET запрос на этот URL возвращает `ok` если блок готов к работе.

Пути URL для различных блоков перечислены ниже:

* Middleware: `/remoting/health`
* Web Client: `/rest/health`
* Web Portal: `/rest/health`

То есть для приложения с именем `app`, развернутого на `++localhost:8080++`, адреса будут следующими:

* http://localhost:8080/app-core/remoting/health
* http://localhost:8080/app/rest/health
* http://localhost:8080/app-portal/rest/health

Ответ `ok` можно заменить на произвольный текст с помощью свойства приложения <<cuba.healthCheckResponse,cuba.healthCheckResponse>>.

Контроллеры проверки посылают <<events,события>> типа `HealthCheckEvent`. Следовательно, вы можете добавить собственную логику проверки работоспособности приложения. В https://github.com/cuba-platform/sample-base/blob/master/modules/web/src/com/haulmont/addon/samplebase/web/HealthCheckListener.java[примере на GitHub], бин web-уровня реагирует на события проверки и вызывает сервис среднего слоя, который в свою очередь выполняет операцию на базе данных.
